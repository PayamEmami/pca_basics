[["index.html", "Principal Component Analysis (PCA) Chapter 1 Prerequisites", " Principal Component Analysis (PCA) Payam Emami 2020-11-09 Chapter 1 Prerequisites In order to run the code in this chapter, you will need to install a number of packages. The packages are listed below. The recommended way of installing the packages is through BioManager. The bookdown package can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) ## ## There is a binary version available but the source version is later: ## binary source needs_compilation ## bookdown 0.20 0.21 FALSE install.packages(&quot;scatterplot3d&quot;) ## ## The downloaded binary packages are in ## /var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T//RtmppEjqyB/downloaded_packages install.packages(&quot;plotrix&quot;) ## ## The downloaded binary packages are in ## /var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T//RtmppEjqyB/downloaded_packages install.packages(&quot;formatR&quot;) ## ## The downloaded binary packages are in ## /var/folders/24/8k48jl6d249_n_qfxwsl6xvm0000gn/T//RtmppEjqyB/downloaded_packages library(scatterplot3d) library(plotrix) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) We will also use a dataset in this chapter. The dataset contains several thousands genes measured on a 23 of samples. The samples are coming from four groups and have been measured in two batches. # read the raw data data&lt;-read.table(&quot;data/b1_b2_data.gct&quot;,sep = &quot;\\t&quot;,header = T,comment.char = &quot;#&quot;) # remove unused columns data&lt;-data[,-c(1:2)] # transpose the data so that genes are in column and samples are in rows data&lt;-t(data) # read the metadata metadata&lt;-read.table(&quot;data/b1_b2_sampleinfofile.txt&quot;,sep = &quot;\\t&quot;,header = T,comment.char = &quot;#&quot;) "],["intro.html", "Chapter 2 Introduction", " Chapter 2 Introduction Biology has become data-intensive as high throughput experiments in genomics or metabolomics have produced datasets of massive volume and complexity. These datasets often contain a huge number of measurements such as genes, transcripts, metabolites, or proteins, measured on some number of samples. Despite that, the scientific questions vary a lot between different studies, but a common question to ask is “what does my data show?”. Obviously, this general question only makes sense in the context of narrower scientific questions. Examples could be: Does my data show any difference between the study groups? Do we have any experimental problems (e.g. batch effect)? Do we have any outliers (e.g. samples that are particularly different from the rest of the cohort)? Do any of the measurements influence the pattern of the data more than others (pattern of the data is generally defined as what data shows! E.g. differences between groups of interest)? Throughout the rest of this chapter, we refer to samples as observations (e.g patients) and measurements as variables (e.g. genes). If we want to summarize the abovementioned questions, we can carefully say that the interest is in variation or simply how the observations are spread. Let’s take one of the variables and see how the data is spread based on this variable: # Select variable variableIndex&lt;-1 # plot the data for variable 1 plot(data[,variableIndex],xlab = &quot;Sample index&quot;,ylab = paste(&quot;Variable&quot;,variableIndex), ylim = c(min(data[,variableIndex])-0.1,max(data[,variableIndex])+0.1),xaxt=&#39;n&#39;) axis(side = 1, at = 1:nrow(data), labels = 1:nrow(data)) # fig position of labels pos_plot&lt;-(as.numeric(data[,variableIndex]&gt;mean(data[,variableIndex]))*2)+1 # Draw sample index text(1:nrow(data), data[,variableIndex], 1:nrow(data), cex=0.65,col=&quot;red&quot;,pos=pos_plot) # draw the mean line abline(h=mean(data[,variableIndex],),col=&quot;red&quot;) # for each observation draw a line from that point to the mean for(i in 1:nrow(data)) { segments(x0 = i,x1 = i,y0 = data[i,variableIndex],y1 = mean(data[,variableIndex]),lty=&quot;dashed&quot;) } Figure 2.1: Here, we select one variable (a gene) and show how the data is spread around the mean. The red numbers show the index of each sample. The red line shows the mean and the dashed lines show the distance between each observation and mean In Figure 2.1, we have plotted the sample number (just a simple index) on the x axis and the expression of variable 1 on the y axis. The red line shows the mean (average) of the variable one and the dashed lines show the distance between each observation (sample) to the mean. The variation is simply the average of squared of these distances to the mean. However, we are still not sure whether the variation we are seen is of any interest for us. Let’s reorder the samples based on the expression. # Select variable variableIndex&lt;-18924 # plot the data for variable 1 # create x axis x_axis&lt;-factor(order(data[,variableIndex]),levels = order(data[,variableIndex])) plot.default(x=x_axis, y=(sort(data[,variableIndex])),xlab = &quot;Sample index&quot;,ylab = paste(&quot;Variable&quot;,variableIndex), ylim = c(min(data[,variableIndex])-0.1,max(data[,variableIndex])+0.1),xaxt=&#39;n&#39;,col=factor(metadata$Covariate)[order(data[,variableIndex])]) # add x axis axis(side = 1, at = 1:nrow(data), labels = x_axis) legend(&quot;topleft&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) # fig position of labels pos_plot&lt;-(as.numeric(sort(data[,variableIndex])&gt;mean(data[,variableIndex]))*2)+1 # Draw sample index text(1:nrow(data), sort(data[,variableIndex]), x_axis, cex=0.65,col=as.numeric(factor(metadata$Covariate)[order(data[,variableIndex])]),pos=pos_plot) # draw the mean line abline(h=mean(data[,variableIndex],),col=&quot;red&quot;) # for each observation draw a line from that point to the mean for(i in 1:nrow(data)) { segments(x0 = i,x1 = i,y0 = sort(data[,variableIndex])[i],y1 = mean(data[,variableIndex]),lty=&quot;dashed&quot;,col= factor(metadata$Covariate)[order(data[,variableIndex])][i]) } Figure 2.2: Here, we select one variable (a gene) and show how the data is spread around the mean. The red numbers show the index of each sample. The red line shows the mean and the dashed lines show the distance between each observation and mean. Please note that the samples are reordered based on expression. the color of points show the grouping of samples Figure 2.1 shows the same variable as Figure 2.2 but we have reorder the observations so that the ones with lower expression come first. We have also added color so that we can see where there is a difference between the groups in this experiment. What is clear now is that, the variation in this variable alone does not provide us with much useful information about all the groups. However, the second group appears to have lower expression compared to the rest of the groups. In addition, The First and the third group show higher expression compared to the rest. That’s great Let’s pick another variable and plot the same thing: # Select variable variableIndex&lt;-20355 # plot the data for variable 2 # create x axis x_axis&lt;-factor(order(data[,variableIndex]),levels = order(data[,variableIndex])) plot.default(x=x_axis, y=(sort(data[,variableIndex])),xlab = &quot;Sample index&quot;,ylab = paste(&quot;Variable&quot;,variableIndex), ylim = c(min(data[,variableIndex])-0.1,max(data[,variableIndex])+0.1),xaxt=&#39;n&#39;,col=factor(metadata$Covariate)[order(data[,variableIndex])]) # add x axis axis(side = 1, at = 1:nrow(data), labels = x_axis) legend(&quot;topleft&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) # fig position of labels pos_plot&lt;-(as.numeric(sort(data[,variableIndex])&gt;mean(data[,variableIndex]))*2)+1 # Draw sample index text(1:nrow(data), sort(data[,variableIndex]), x_axis, cex=0.65,col=as.numeric(factor(metadata$Covariate)[order(data[,variableIndex])]),pos=pos_plot) # draw the mean line abline(h=mean(data[,variableIndex],),col=&quot;red&quot;) # for each observation draw a line from that point to the mean for(i in 1:nrow(data)) { segments(x0 = i,x1 = i,y0 = sort(data[,variableIndex])[i],y1 = mean(data[,variableIndex]),lty=&quot;dashed&quot;,col= factor(metadata$Covariate)[order(data[,variableIndex])][i]) } Figure 2.3: Here, we select one variable (a gene) and show how the data is spread around the mean. The red numbers show the index of each sample. The red line shows the mean and the dashed lines show the distance between each observation and mean. Please note that the samples are reordered based on expression. the color of points show the grouping of samples As it’s evident in Figure 2.3, the variation in this variable shows us the difference between the fourth group and the rest but it does not provide much information about the the rest of the groups. Let’s have a look at what information both of these variable give. We simply put both figures Figure 2.2 and Figure 2.3 together: par(mfrow=c(2,1)) # Select variable variableIndex&lt;-18924 # plot the data for variable 1 # create x axis x_axis&lt;-factor(order(data[,variableIndex]),levels = order(data[,variableIndex])) plot.default(x=x_axis, y=(sort(data[,variableIndex])),xlab = &quot;Sample index&quot;,ylab = paste(&quot;Variable&quot;,variableIndex), ylim = c(min(data[,variableIndex])-0.1,max(data[,variableIndex])+0.1),xaxt=&#39;n&#39;, col=factor(metadata$Covariate)[order(data[,variableIndex])]) # add x axis axis(side = 1, at = 1:nrow(data), labels = x_axis) legend(&quot;topleft&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) # fig position of labels pos_plot&lt;-(as.numeric(sort(data[,variableIndex])&gt;mean(data[,variableIndex]))*2)+1 # Draw sample index text(1:nrow(data), sort(data[,variableIndex]), x_axis, cex=0.65,col=as.numeric(factor(metadata$Covariate)[order(data[,variableIndex])]),pos=pos_plot) # draw the mean line abline(h=mean(data[,variableIndex],),col=&quot;red&quot;) # for each observation draw a line from that point to the mean for(i in 1:nrow(data)) { segments(x0 = i,x1 = i,y0 = sort(data[,variableIndex])[i],y1 = mean(data[,variableIndex]),lty=&quot;dashed&quot;,col= factor(metadata$Covariate)[order(data[,variableIndex])][i]) } # Select variable variableIndex&lt;-20355 # plot the data for variable 2 # create x axis x_axis&lt;-factor(order(data[,variableIndex]),levels = order(data[,variableIndex])) plot.default(x=x_axis, y=(sort(data[,variableIndex])),xlab = &quot;Sample index&quot;,ylab = paste(&quot;Variable&quot;,variableIndex), ylim = c(min(data[,variableIndex])-0.1,max(data[,variableIndex])+0.1),xaxt=&#39;n&#39;, col=factor(metadata$Covariate)[order(data[,variableIndex])]) # add x axis axis(side = 1, at = 1:nrow(data), labels = x_axis) legend(&quot;topleft&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) # fig position of labels pos_plot&lt;-(as.numeric(sort(data[,variableIndex])&gt;mean(data[,variableIndex]))*2)+1 # Draw sample index text(1:nrow(data), sort(data[,variableIndex]), x_axis, cex=0.65,col=as.numeric(factor(metadata$Covariate)[order(data[,variableIndex])]),pos=pos_plot) # draw the mean line abline(h=mean(data[,variableIndex],),col=&quot;red&quot;) # for each observation draw a line from that point to the mean for(i in 1:nrow(data)) { segments(x0 = i,x1 = i,y0 = sort(data[,variableIndex])[i],y1 = mean(data[,variableIndex]),lty=&quot;dashed&quot;,col= factor(metadata$Covariate)[order(data[,variableIndex])][i]) } Figure 2.4: Here, we select two previous variables and show how the data is spread around the mean. The red numbers show the index of each sample. The red line shows the mean and the dashed lines show the distance between each observation and mean. Please note that the samples are reordered based on expression. the color of points show the grouping of samples We can see in Figure 2.4, there is obviously some information that is almost only explained by individual variables. For example the differences between group 2 and group 4 is mostly explained by variable 20355. But there is also some small information that seems to be redundant. For example the differences between combined groups 1 and 3 vs group 4. We can have a look at variation and redundancy together by plotting the expressions of both variables against each other. plot(data[,c(18924,20355)],col=factor(metadata$Covariate)) # define variables var1&lt;-18924 var2&lt;-20355 # plot the data plot(data[,c(var1,var2)],col=factor(metadata$Covariate),xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 2&quot;) # create legend legend(&quot;topleft&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) # fig position of labels pos_plot&lt;-(as.numeric(sort(data[,variableIndex])&gt;mean(data[,variableIndex]))*2)+1 # Draw sample index text((data[,var1]), (data[,var2]), 1:nrow(data), cex=0.65,col=as.numeric(factor(metadata$Covariate)),pos=pos_plot) Figure 2.5: Here, we select two variables and show how the data is spread according to both of the variables. Figure 2.5 is called 2d scatter plot that is used to show the relationships between two variables. It’s now more evident that we can separate group 2 and 4 more easily. This means the combination of two variables gave us more information than a single variable at the time. At this time we can more formally say that we have two axis x and y (Variable 1 and 2) which define our data. Each axis has some information. To quantify that information, let’s agree that the amount of information in each axis is measured by variation of the observations in that axis. As said before, the variation is define as the the average squared distances from the mean. That can be easily computer using a simple R function (var): # calculate variation of variable 1 variation_var1&lt;-var(data[,var1]) # calculate variation of variable 2 variation_var2&lt;-var(data[,var2]) sprintf(&quot;Variation of variable1: %f Varition of variable2: %f&quot;,variation_var1,variation_var2) ## [1] &quot;Variation of variable1: 3.282278 Varition of variable2: 2.753621&quot; In the beginning it might sound to use variation to quantify information as one variable can contain huge noise level that can easily be higher than another variable that is more informative. However, in the presence of no addition information (such as groups in this example), we can at least assume (hope) that if the experiment has been done perfectly the variation of information is higher than variation of noise. So in this case, we can say that variable 1 has more information than variable 2. We also mentioned that any two variables can possibly contain redundant information. To quantify that, we use a method that is called covariance. Covariance measures to what extend two variables are showing the same information. To calculate the covariance, for each of the observations (samples), we calculate difference of that observation to the mean for variable 1, we do the same thing for variable 2 and then multiply this the resulting numbers number. Once we did that for all the observations, we sum all the numbers and divide the results by total number of observations. The sign of covariance tells whether the two variables are positively or negatively related. The magnitude of covariance can be thought as total variation or the amount of redundancy between two samples. The higher the absolute value the higher the redundancy (we go through this in detail in the next chapters). The covariance of zero or close to zero means we have no or little redundancy. Let’s try to see what’s visually happening: # define variables var1&lt;-18924 var2&lt;-20355 # plot the data plot(data[,c(var1,var2)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 2&quot;) # fig position of labels pos_plot&lt;-(as.numeric(sort(data[,variableIndex])&gt;mean(data[,variableIndex]))*2)+1 # Draw sample index text((data[,var1]), (data[,var2]), 1:nrow(data), cex=0.65,pos=pos_plot) # plot mean of variable 1 abline(v=mean(data[,var1]),col=&quot;purple&quot;,lty=1) # plot mean of variable 2 abline(h=mean(data[,var2]),col=&quot;coral&quot;,lty=1) for(i in 1:nrow(data)) { segments(x0 = (data[,var1])[i],x1 = mean(data[,var1]),y0 = (data[,var2])[i],y1 = (data[,var2])[i],lty=&quot;dashed&quot;, col=ifelse(data[i,var1]&gt;mean(data[,var1]),yes = &quot;indianred1&quot;,no = &quot;lightblue&quot;)) segments(x0 = (data[,var1])[i],x1 = (data[,var1])[i],y0 = (data[,var2])[i],y1 = mean(data[,var2]),lty=&quot;dashed&quot;, col=ifelse(data[i,var2]&gt;mean(data[,var2]),yes = &quot;indianred1&quot;,no = &quot;lightblue&quot;)) } # add areas to the plot text(4,8,&quot;Area 1&quot;) text(10,8,&quot;Area 2&quot;) text(10,2.5,&quot;Area 3&quot;) text(4,2.5,&quot;Area 4&quot;) Figure 2.6: Here, we select two variables and show how the data is spread according to both of the variables. The solid purple and coral lines show mean of variable and variable 2, respectively. For each observation, we have two lines showing whether it has higher or lower value compared to mean of variable one and variable 2. In Figure 2.6, we have plotted the mean of variable 1 using the vertical purple line and mean of the variable 2 using horizontal coral line. For each observation we have two lines, vertical line shows the difference of that observation to mean of variable 1 and horizontal line show the difference to the mean of variable 2. If the line is red, it means the difference is higher than zero and if the mean is blue, it means the difference is negative thus lower than zero. If an observation lays in areas 1 or 3, it means that it behaves oppositely (relative to the mean: solid lines) in variable 1 and 2. This means if the expression goes up in variable 1, it goes down in variable 2 (or vice versa). However, if an observation lays in areas 2 or 4, it means its behavior is consistent in both of the variables. If most observation end of being in areas 2 and 4, we will have a positive covariance, meaning that they change consistently both in variable 1 and variable 2. If most of them are in area 1 and 3, we will have a negative covariance, meaning that they change in opposite directions. If the observations lay in all the four areas with approximately the same magnitude (distance to the mean: length of the red and blue lines), we will have a low or no covariance. Please note that, the magnitude of the distances to the mean is also very important. For example even if all the observations end up in the areas 1 and 3 but one observation lays in area 2 with a very large distance from the means, we could still get a large positive covariance. In R the covariance is calculated using “cov” function: # calculate variation of variable 1 covariation_var1_2&lt;-cov(data[,var1],data[,var2]) sprintf(&quot;Covariation of variable1 and variable 2 is: %f&quot;,covariation_var1_2) ## [1] &quot;Covariation of variable1 and variable 2 is: 0.172791&quot; We see that we got a low covariance which means that we have small redundancy in the two variables. This was expected just by looking at the Figure 2.6. Most of our observations ended up being scattered in all four ares with very similar magnitudes. This might be exiting as we have two variables that are showing complimentary information. We did not waste our resources on measuring the things that are showing the same information. However, we have bigger issue here! We just took a look at two of the variables out of 45101 measured ones! We specifically selected these variable to show this information. Look at this new variable: # define number of plots in the figure par(mfrow=c(1,3)) # define variables var1&lt;-18924 var2&lt;-20355 var3&lt;-18505 # plot the data variable 1 vs 3 plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,col=factor(metadata$Covariate)) # create legend legend(&quot;topleft&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) # plot the data variable 2 vs 3 plot(data[,c(var2,var3)],xlab = &quot;Variable 2&quot;, ylab = &quot;Variable 3&quot;,col=factor(metadata$Covariate)) # load 3d scatter plot library library(&quot;scatterplot3d&quot;) # plot the data scatterplot3d(data[,c(var1,var2,var3)],xlab = &quot;Variable 1&quot;,ylab=&quot;Variable 2&quot;,zlab = &quot;Variable 3&quot;, angle = 10,color=as.numeric(factor(metadata$Covariate))) Figure 2.7: Here, we select one of the previous variables and an additional one and show how the data is spread according to both of the variables. The covariance of this new variable with the old ones is variable one: 3.4341536 and variable 2: 1.4907196. It is also obvious from the Figure 2.7 these new variable contain redundant information. What about others? how kind of information do they contain? which ones are redundant which ones are complimentary? If we have to do the same analysis on the entire dataset, at best, we can repeat our analysis for three variables at a time, meaning that we will end up doing that 15288975159150 times! And how about if we should use more than 3 variables at the time to inspect the data pattern? In fact, as the dimension of data (number of variables, e.g. genes) increases, it becomes very difficult to draw conclusion from the data. We just cannot do that using simple methods. Fortunately, we have some amazing methods coming to rescue! This is the topic of the next chapter. "],["pcamot.html", "Chapter 3 PCA (motivation) 3.1 Summary of the previous chapter 3.2 Variation and covariation 3.3 Finding the largest variation 3.4 PCA (freindly definition)", " Chapter 3 PCA (motivation) 3.1 Summary of the previous chapter In the previous chapter, we talked about two measures for summarizing our data, variation and covariation. Variation was used to measure the spread of a single variable whereas the covariation was between two variables measuring how much they agree with each other, thus showing the redundancy. We also emphasized that it is nice to have variables showing complimentary information rather than overlapping one! We also decided that, we want the variation of signal to be higher than the variation of noise. That is, probably the variables with higher variation are more interesting to us. We also show that the combination of two variables can give us more information about the data pattern, meaning that each of the variable contribute to the overall pattern. So to summarize: We agree that, we are interested in variation We hope that the variation of signal is higher noise By combining more variables together, we can probably get more information There are redundancies between the different variables but we are mostly interested in the complimentary information they provide and to a lesser extend to the redundant information We cannot manually handle a huge number of variable at the same time 3.2 Variation and covariation This is a small point but so important that we put it under a separate heading. Please note that variation and covariation are unbiased estimates. This means that, they do not take into account the groupings/phenotype/meta data/other information about observations (samples). And we are going to agree that if variable A (a gene) has a variation of 2 and another variable B has a variation 1. We want to remain unbiased and give higher value to variable A compared to variable B EVEN if B shows us more “true” information about our experiment. So the higher the variation, the better, no matter what it shows. This might sound counterintuitive, but we will show that, remaining unbiased, will show us the data pattern that otherwise could not have been found. So as of now, we are not going to show you labels of samples until when we want to interpret the data pattern Remember: the higher the variation, the better Variation of the data is how the data is spread, the bigger the spread the larger the variation 3.3 Finding the largest variation Let’s start working on the same dataset. In the previous chapter, we worked on maximum of three variables at the time (three axis: variable 1, 2, and 3). Twos variables showed high covariance. Let’s plot them again. This time, without any colors! # fix number of columns in the plot par(mfrow=c(1,1)) # define variables var1&lt;-18924 var3&lt;-18505 # plot the data plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;) Figure 3.1: Here, we select two variables and show how the data is spread according to the variables. In this figure, we have two axis, x: variable 1 and y: variable 3. These two variables have high covariance (they change together). Each axis is obviously a line in which each of our observations has a location (or projection). For example for axis x: # fix number of columns in the plot par(mfrow=c(1,2)) # define variables var1&lt;-18924 var3&lt;-18505 # plot the data plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,axes = F) # draw the box box(col = &#39;black&#39;) # draw x axis axis(1, col = &#39;red&#39;,cex=4) # draw y axis axis(2) # plot the arrows for(i in 1:nrow(data)) { arrows(x0 =as.numeric( data[i,var1]),x1 = data[i,var1], y0 = data[i,var3],y1 =2.85,length=0.05,lty=&quot;dashed&quot;) } # plot the data plot(data[,c(var1)],y=rep(0,nrow(data)),xlab = &quot;Variable 1&quot;, ylab = &quot;&quot;,axes = F) # plot the axis axis(1, col = &#39;red&#39;,cex=4,pos = c(0,0),at =unique(c(floor(data[,c(var1)]),ceiling(data[,c(var1)]))) ) Figure 3.2: Here, we select two variables and show how the data is spread according to the variables. We show how the data is projected on the fist axis We can see in Figure 3.2 (left panel), all of our observations have been mapped (projected) to the axis x (variable 1). The result is shown on the right plot. Obviously, it gives us the original values of observations in variable 1 with variation of 3.2822776. We could do the same thing for variable 3. # fix number of columns in the plot par(mfrow=c(1,2)) # define variables var1&lt;-18924 var3&lt;-18505 # plot the data plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,axes = F) # draw the box box(col = &#39;black&#39;) # draw y axis axis(2, col = &#39;red&#39;,cex=4) # draw x axis axis(1) for(i in 1:nrow(data)) { arrows(x0 =as.numeric( data[i,var1]),x1 = 3.5, y0 = data[i,var3],y1 =data[i,var3],length=0.05,lty=&quot;dashed&quot;) } # plot the data plot(x=rep(0,nrow(data)),y=data[,c(var1)],xlab = &quot;&quot;, ylab = &quot;Variable 3&quot;,axes = F) #plot the axis axis(2, col = &#39;red&#39;,cex=4,pos = c(0,0),at =unique(c(floor(data[,c(var3)]),ceiling(data[,c(var3)]))) ) Figure 3.3: Here, we select two variables and show how the data is spread according to the variables. We show how the data is projected on the second axis This axis in Figure 3.3 gave us variation of 6.3821145. The covariance of these two axis is 3.4341536. So to summarize, we draw two lines (axis) and mapped (projected the data points to these lines and measured the variance and covariance of the mapped data. Well, we are not limited to these lines! We can draw other lines. The idea is to come up with a line drawn through the data in a way that the projected data (orthogonal projection) has maximum variation. Let’s try to see what it means in action by randomly draw a line # fix number of columns in the plot par(mfrow=c(1,2)) # define variables var1&lt;-18924 var3&lt;-18505 # plot the data plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,xlim = c(2,15),ylim = c(2,15)) # draw a line # intercept b&lt;-2.5 # slope m&lt;-0.5 # draw the line abline(a = b,b = m,col=&quot;red&quot;) # projected_variables&lt;-matrix(NA,nrow = nrow(data),ncol = 2) for(i in 1:nrow(data)) { # calculate the projection l2 &lt;- c(1, m + b) l1 &lt;- c(0, b) l1 &lt;- c(l1, 0) l2 &lt;- c(l2, 0) u &lt;- sum((c(data[i,c(var1,var3)],0) - l1)*(l2 - l1)) / sum((l2 - l1)^2) r&lt;-l1 + u * (l2 - l1) # end projection # draw arrow arrows(x0 = data[i,var1],x1 = r[1], y0 = data[i,var3],y1 =r[2],length=0.05,lty=&quot;dashed&quot;) # save the projections projected_variables[i,1]&lt;-r[1] projected_variables[i,2]&lt;-r[2] } plot(projected_variables,xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,xlim = c(2,15),ylim = c(2,15)) abline(a = b,b = m,col=&quot;red&quot;) Figure 3.4: Here, we select two variables and show how the data is spread according to the variables. We draw a line through the data and project the data on that line and then measure the variance. #plot(x=apply(projected_variables,1,mean),y=rep(0,nrow(data))) So in Figure 3.4 we have drawn an arbitrary line (left panel) and put everything on that line (right panel). It might be a bit confusing. But let’s rotate that red line so it becomes horizontal exactly line x axis. # set number of figures in a plot par(mfrow=c(1,1)) plot(x=apply(projected_variables,1,mean),y=rep(0,nrow(data)),axes = F,ylab = &quot;&quot;,xlab = &quot;Combined variable 1 and 3&quot;) axis(1, col = &#39;red&#39;,cex=4,pos = c(0,0),at =unique(c(floor(apply(projected_variables,1,mean)),ceiling(apply(projected_variables,1,mean)))) ) Figure 3.5: Simple rotation of the projected points on the line Figure 3.5 shows our new x axis. If fact, this is a new variable! It’s not quite variable 1, it’s not variable 3, but a combination of these two. What is the variation of this new variable?! The variation is: 2.9923055. Well, that is great! But the question is, we can draw as many lines as we want, what should we do? Let’s, imagine that we can try infinite possibilities. Which line would you select? # fix number of columns in the plot par(mfrow=c(1,3)) # define variables var1&lt;-18924 var3&lt;-18505 # plot the data plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,xlim = c(2,15),ylim = c(2,15)) # draw a line # intercept b&lt;-2.5 # slope m&lt;-0.5 # draw many lines using different slope and intercept for(b in c(1:5)) { for(m in seq(0,1,by=0.1)) { abline(a = b,b = m,col=&quot;red&quot;) } } # intercept b&lt;--5.172751 # slope m&lt;-1.548455 # draw the line abline(a = b,b = m,col=&quot;blue&quot;,lwd = 2) plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,xlim = c(2,15),ylim = c(2,15)) # draw the line abline(a = b,b = m,col=&quot;blue&quot;) # projected_variables&lt;-matrix(NA,nrow = nrow(data),ncol = 2) for(i in 1:nrow(data)) { # calculate the projection l2 &lt;- c(1, m + b) l1 &lt;- c(0, b) l1 &lt;- c(l1, 0) l2 &lt;- c(l2, 0) u &lt;- sum((c(data[i,c(var1,var3)],0) - l1)*(l2 - l1)) / sum((l2 - l1)^2) r&lt;-l1 + u * (l2 - l1) # end projection # draw arrow arrows(x0 = data[i,var1],x1 = r[1], y0 = data[i,var3],y1 =r[2],length=0.05,lty=&quot;dashed&quot;) # save the projections projected_variables[i,1]&lt;-r[1] projected_variables[i,2]&lt;-r[2] } # save projection direction (came from svd) projection_directions&lt;-matrix(c(-0.5425087359 ,-0.8400501601,-0.8400501601,0.5425087359),nrow = 2) # decenter data decenter_data&lt;-scale((scale(data[,c(var1,var3)],scale = F))%*%projection_directions, scale = FALSE, center = -1 * c(7.767432,6.854765)) # plot the data plot(x=decenter_data[,1],y=rep(0,nrow(data)),axes = F,ylab = &quot;&quot;,xlab = &quot;Combined variable 1 and 3&quot;) axis(1, col = &#39;blue&#39;,cex=4,pos = c(0,0),at =unique(c(floor(decenter_data[,1]),ceiling(decenter_data[,1]))) ) Figure 3.6: We try to fit many lines through the data but there is only one of the lines that capture most of the variance Intuitively, one would select a line that has the maximum variation. That is the blue line. Variation of this line is 8.5999087 that is the highest amount all other lines that can be drawn. By now, we have a new variable that is a combination the original two variables and has captured most of the variation. But how about the remaining variation? What should we do about the redundancy? So ideally what we want to do is to come up with another line that when we mapped the data on it, it captures the remaining variation but also avoid redundancy. Last point means that the covariance between the observations that we have already mapped to the first line (the blue line in Figure 3.6) and the observations that we mapped on this new line should be zero or very close to zero. It turns out that the only way of finding such a line is to find the line that is orthogonal to the first line (the blue line in Figure 3.6). Orthogonal means that the angle of this new line to the blue on should be exactly 90 degree: # set number of plots to 1 par(mfrow=c(1,1)) # plot an empty plot plot(1,type=&quot;n&quot;,axes=F,xlim=c(-0.1,1.2),ylim=c(-0.1,1.2),xlab=&quot;&quot;,ylab=&quot;&quot;) # define axis x1 = c(1,0) x2 = c(0,1) # plot x arrow arrows(0,0,x1[1],x1[2]) # plot y arrow arrows(0,0,x2[1],x2[2]) # draw angle segments(x0 = 0.1,y0 = 0,x1 = 0.1,y1 = 0.1) segments(x0 = 0,y0 = 0.1,x1 = 0.1,y1 = 0.1) # draw arrow to the text arrows(0,0,0.17,0.17) # write text text(0.4,0.2,&quot;This angle is 90 degree&quot;,cex=1.5) Figure 3.7: We try to fit many lines through the data Figure 3.7 shows two arbitrary lines, the lines are called orthogonal/perpendicular to each other. So now that we know what orthogonal means, we will find a line to be orthogonal to our blue line in Figure 3.6): # fix number of columns in the plot par(mfrow=c(1,2)) # define variables var1&lt;-18924 var3&lt;-18505 # plot the data plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,xlim = c(2,15),ylim = c(2,15)) # draw a line # intercept b&lt;--5.172751 # slope m&lt;-1.548455 # draw the line abline(a = b,b = m,col=&quot;blue&quot;,lwd = 2) # intercept b&lt;-11.8710127 # slope m&lt;--0.6458052 # draw the line abline(a = b,b = m,col=&quot;green&quot;,lwd = 2) # projected_variables&lt;-matrix(NA,nrow = nrow(data),ncol = 2) for(i in 1:nrow(data)) { # calculate the projection l2 &lt;- c(1, m + b) l1 &lt;- c(0, b) l1 &lt;- c(l1, 0) l2 &lt;- c(l2, 0) u &lt;- sum((c(data[i,c(var1,var3)],0) - l1)*(l2 - l1)) / sum((l2 - l1)^2) r&lt;-l1 + u * (l2 - l1) # end projection # draw arrow arrows(x0 = data[i,var1],x1 = r[1], y0 = data[i,var3],y1 =r[2],length=0.05,lty=&quot;dashed&quot;) # save the projections projected_variables[i,1]&lt;-r[1] projected_variables[i,2]&lt;-r[2] } # plot the data plot(x=decenter_data[,2],y=rep(0,nrow(data)),axes = F,ylab = &quot;&quot;,xlab = &quot;Combined variable 1 and 3&quot;) axis(1, col = &#39;green&#39;,cex=4,pos = c(0,0),at =unique(c(floor(decenter_data[,2]),ceiling(decenter_data[,2]))) ) Figure 3.8: We try to fit the second best line through the data Great! Now we have two new axis. The variance of the first one (blue) was 8.5999087 and the second one was 1.0644835. And most importantly, the covariance (redndancy) betweem these two new axis is 3.556493910^{-10}. That is almost zero. Perfect! We have two new variables that are showing complementary variation and no redundancy. Please also note that, our small two variable dataset had a total variation of 9.6643921 and our new variables also have 9.6643921. So we did not remove anything! We just rotate the data along two new axis (blue and green) such that our first axis (blue) show us most of the variation and does not have covariance with our second variable. Now it’s time to plot both the new variables and see what they show us: # fix number of columns in the plot par(mfrow=c(1,2)) # define variables var1&lt;-18924 var3&lt;-18505 # plot the data plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;, ylab = &quot;Variable 3&quot;,col=factor(metadata$Covariate),xlim=c(2,14),ylim=c(2,12)) title(&quot;Original variables&quot;) # add legend legend(&quot;topleft&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) plot(decenter_data,xlab = &quot;New variable 1&quot;, ylab = &quot;New variable 2&quot;,col=factor(metadata$Covariate),xlim=c(2,14),ylim=c(2,12),axes = F) box() axis(1, col = &#39;blue&#39; ) axis(2, col = &#39;green&#39; ) title(&quot;New variables&quot;) Figure 3.9: Plot of the original variables together with the new axis It should be clear by now that we just rotated the data. But there is one more thing to this plot, in the left, the differences between groups are demonstrated by the values of two variables together (variable 1 and variable 3). However, in the panel on the right, the differences between the groups are mostly visible in just one variable (New variable 1, the blue line). So if we see the variation of interest in one variable, what is the reason to keep the other one? We can simply remove the New variable 2 and represent our data with only one variable. # fix number of columns in the plot par(mfrow=c(1,1)) # plot the data plot(x=decenter_data[,1],y=rep(1.2,nrow(data)),axes = F,ylab = &quot;&quot;,xlab = &quot;New variable 1&quot;,col=factor(metadata$Covariate)) axis(1, col = &#39;blue&#39;,cex=4,pos=c(1,1),at =unique(c(floor(decenter_data[,1]),ceiling(decenter_data[,1]))) ) # add legend legend(&quot;topleft&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) Figure 3.10: Plot of the data with one of the variables removed We can see in Figure 3.10 that the differences are represented in one variable what is the combined effect of the two original variables. This might not sound such a big deal as one of our original variables (Variable 1) already represented the data very well. In addition, when removing the new variable 2, in fact we only remove 11 percent of the variation. Whereas if we remove either of the original variable 1 or 2, we will remove 34 or 66 percent respectively. Meaning that using the aforementioned method, we concentrated the variation in a limited number of variables. So removing the variable with little variation should not harm the overal data pattern. Reducing from two variables to one is not such a big deal! But how about reducing from 45101 variables to just a few ones? This is exactly what PCA does! 3.4 PCA (freindly definition) PCA does exactly what we have been doing so far but it can do it for much much more complex data with thousands of variables (e.g. genes)! Its job is to find new variables from the combination of original variables and then sort them. It does that in a way that, after sorting, the first variable has the most variation (e.g. information), the second one has the second most and so forth. And all this amazing thing comes with the luxury of not having redundant variation between the new variables. All these words mean that PCA will take your data, and rotate them and show you the angle of the data that you will see most of the differences between your observations (e.g. samples/patients/data points etc). It does this unbiased, without knowing what you are interested in. It simply does not care! For PCA higher variation is better, it will simply show it first. That is exactly the reason why it’s a fantastic method for finding hidden pattern in the data. We will go through a few examples in the next chapter. In short, you can do the following with PCA: Visualization of high dimensional data: dimensions are normally the number of variables (e.g. genes) that are in the dataset Dimentionality reduction: exactly like we did here for removing variables Variable selection: selecting which of the variables (e.g. genes) has most influence on the overal data pattern Batch removing: Batch effect or other experimental biases can be removed by PCA We are going to go through all the abovementinoed points in the next chapter. See you! "],["pcaapplications.html", "Chapter 4 PCA (applications) 4.1 Summary of the previous chapter 4.2 PCA in R 4.3 Centering 4.4 Scaling 4.5 Visualization of data and dimentionality reduction 4.6 Variable selection 4.7 Batch removing (removing variation)", " Chapter 4 PCA (applications) 4.1 Summary of the previous chapter We have already discussed what PCA does. This method is used to create a set of new variables that are combination of the original variables. PCA then sort these variables in descending order with respect to their variation. The total variation of all new variables are always the same as the original ones. The new variables will be always orthogonal (no covariance, no redundancy). Simply talking, PCA rotates the data and shows a particular view of the data such that the user can see where most of the variation is sitting! One can later decide where this variation is of interest or not depending on the research question. At this stage, we would like to remind you about the dataset we are using. In our data we have 23 observations (e.g. samples/patients) and 45101 variables (e.g. measurements such as genes). The observation are coming from 4 groups: group 1, group 2, group 3, group 4 (e.g. cancer sub-types). We have been coloring our points in the plot based on these groups. Finally this experiment was run in 2 batches (in the lab!). So let’s get started by doing some PCA! 4.2 PCA in R We are going to use prcomp function in R to perform PCA. The main reason for selecting this function is simple. This is a function that is coming default with R, gives decent accuracy and is more or less accepted at the default method of doing a PCA in R. However, you should note that there are many more ways of doing a PCA in R, some of which provide much more functionalities compared to prcomp. Before proceeding with using prcomp, it is important to organize your data correctly: From now on, your data is a matrix (like table! although it’s sloppy to say that!) Your observations (e.g. samples) are in rows Your variables (e.g. measurements) are in column If you ever wanted to use other functions, please do check in what format they need the data. This is important So to give you an example, we show you part of our data: # show 10 variables and all the samples knitr::kable(as.data.frame(round(data[,1:5],2)),caption = &quot;An example matrix where samples are in the rows and variables are in column&quot;) Table 4.1: An example matrix where samples are in the rows and variables are in column V1 V2 V3 V4 V5 Sample1_Group1_Batch1 1.67 1.85 1.53 1.39 1.83 Sample2_Group1_Batch1 1.44 1.71 1.76 1.53 2.06 Sample3_Group1_Batch1 1.33 1.77 1.76 1.53 1.76 Sample4_Group1_Batch1 1.54 1.95 1.73 1.48 1.76 Sample5_Group3_Batch1 1.52 1.88 1.63 1.40 1.73 Sample6_Group3_Batch1 1.76 1.65 1.82 1.53 1.74 Sample7_Group3_Batch1 1.63 1.77 1.84 1.48 1.88 Sample8_Group3_Batch1 1.59 1.65 1.63 1.58 1.69 Sample9_Group3_Batch1 1.41 1.67 1.65 1.49 1.76 Sample10_Group2_Batch1 1.54 1.77 1.53 1.48 1.63 Sample11_Group2_Batch1 1.57 2.17 1.53 1.47 1.76 Sample12_Group2_Batch1 1.47 1.79 1.83 1.47 1.56 Sample13_Group2_Batch1 1.53 1.49 1.43 1.37 1.78 Sample14_Group4_Batch1 1.50 1.74 1.58 1.50 1.77 Sample15_Group4_Batch1 1.70 1.98 1.48 1.56 1.82 Sample16_Group1_Batch2 2.17 2.49 2.40 2.28 2.54 Sample4_Group1_Batch2 2.08 2.55 2.25 2.09 2.55 SampleA_Group2_Batch2 2.28 2.71 2.44 2.26 2.35 Sample10_Group2_Batch2 2.28 2.50 2.18 2.17 2.34 Sample5_Group3_Batch2 2.20 2.48 2.27 2.17 2.33 SampleB_Group4_Batch2 2.08 2.38 2.44 2.21 2.36 SampleCGroup4_Batch2 2.02 2.31 2.20 2.16 2.43 SampleD_Group4_Batch2 1.89 2.10 2.11 2.14 2.14 As you see in Table 4.1, the observations are in rows (e.g. Sample1_Group1_Batch1) and variables are in columns (e.g. V1). By now you should now how to use functions in R. However, for just a brief reminder, a function is like a small program and does something. Exactly like in Microsoft Word where you click on some menus to for example to change the font. You then need to select the font, style, size etc. These options, we tend to call them parameters or arguments. Normally functions in R need a few parameters in order to do exactly what you want. prcomp is no exception! Fortunately, the number of parameters that we will use is not too many: x: is your data matrix center: Shifts the variables to have zero mean (Don’t panic! See below) scale: Scales the variables to have unit variance (Scary huh?! See below) That was it. We won’t use the rest of the parameters! You actually don’t need them unless you are doing something very special! The way that we run a function in R is to write name of the function then parenthesis open, add parameters (separated by comma) and parenthesis close! So for running a PCA we can do for example: prcomp(x=x,center=TRUE,scale. = TRUE) This is normally reffered to as calling a function. It says, run a pca, with parameters, x equals to x, center is true and scale is also true. Many functions in R give the results of their jobs some form. We normally say, they return something. So we can even be cooler and save the retuned results somewhere so we can use it. pcaResults &lt;- prcomp(x=x,center=TRUE,scale. = TRUE) As said before, x is your data matrix that we have already prepared. 4.3 Centering We have a set of variables (e.g genes) in our data. If we set centering to TRUE, R, first, will calculate average of each variable and then subtract this average from the values of the variable (for each observation, e.g. sample). In short, it will shift the data so that the center of our data is zero. We can see an example of this for ten of our variables: # set number of figures par(mfrow=c(1,2)) # show 10 variables and all the samples boxplot(data[,1:10],xlab=&quot;Variable&quot;,ylab=&quot;Variable value (e.g. expression)&quot;) title(&quot;Original data&quot;) # calculate the means means &lt;- apply(data[,1:10],2,mean) #plot the means points(x=1:10,y = means,col=&quot;red&quot;,pch=18) # show 10 variables and all the samples but centered boxplot(scale(data[,1:10],center = TRUE, scale = FALSE),xlab=&quot;Variable&quot;, ylab=&quot;Variable value (e.g. expression)&quot;) title(&quot;Centered data&quot;) # calculate the means means &lt;- apply(scale(data[,1:10],center = TRUE, scale = FALSE),2,mean) #plot the means points(x=1:10,y = means,col=&quot;red&quot;,pch=18) Figure 4.1: Boxplot of ten variables before and after centering Figure 4.1 shows boxplots of ten variables in our data. The left panel is showing the original data where the mean of each variable has been showing in red dimonds. In the right panel, we see that after centering the mean of all variables become zero. We will go through this in detail later. for now: IMPORTANT: You MUST center your data for doing PCA. If your data is not centered, you are NOT doing PCA. Setting the center parameters to TRUE will force prcomp to center your data automatically. DO NOT set this to FALSE if you are not sure whether your data is centered 4.4 Scaling Many datasets typically come in different scales and this happens even within a dataset. That means that if i measures three variables (e.g genes), the scale of the measurements can be different between these three. This is specially the case for high throughput experiments where thousands of variables are measured at the same time. A mini example would be one person measuring height in centimeter and another one measuring height in meter. Scaling is a way of forcing your variables to show one unit scale. One common of way of doing this, is to calculate standard deviation (square root of variance) of each variable, and divide the variable by this standard deviation. Let’s see if we can demonstrate what it means with an example: # set number of figures par(mfrow=c(1,2)) # create two variables varibale1&lt;-data[,var1] varibale2&lt;-data[,var1]*10 # create a data frame dataDummy&lt;-data.frame(Variable1=varibale1,Variable2=varibale2) # show 10 variables and all the samples boxplot(dataDummy,xlab=&quot;Variable&quot;,ylab=&quot;Variable value (e.g. expression)&quot;) # plot scatter plot(dataDummy) Figure 4.2: Example of two variables with different scales In Figure 4.2(left panel) which one of these two variables has most of the variation? Well you are right! Variable2 has 328.2277629 whereas Variable1 has 3.2822776. Remember what PCA shows: the angle with most of the variation. Obviously, this is going to be Variable2. There is one point and i guess you also see it in the code. These two variables are identical except that Variable2 equals to Variable1 multiply by 100. You can see that in the scatter plot in the right panel. These two variables are identical. Let’s see the effect of scalling: # set number of figures par(mfrow=c(1,2)) # create two variables varibale1&lt;-data[,var1] varibale2&lt;-data[,var1]*10 # create a data frame dataDummy&lt;-data.frame(Variable1=varibale1,Variable2=varibale2) # scale the data, not center dataDummy&lt;-scale(dataDummy,center = FALSE, scale = TRUE) # show 10 variables and all the samples boxplot(dataDummy,xlab=&quot;Variable&quot;,ylab=&quot;Variable value (e.g. expression)&quot;) # plot scatter plot(dataDummy) Figure 4.3: Example of two variables which are scalled Perfect! right?! Now we see that they are identical and obviously have the same variation (0.0494634). So we were fooled by scales. If we have thousands of variables and just one or two of them are coming from a different scales compared to the rest, PCA would also have been fooled and showed us only those two variables! So the point is: If you don’t know your variables are in the same scale, set scale to TRUE The rule of thumb is that, if you don’t know anything about your data, set both scale and center to TRUE. You will be probably safe :) Now it’s time to use PCA to do something. # do pca pcaResults&lt;-prcomp(data,scale. = T,center = T) # convert pca result to dataframe pcaResults_table&lt;-data.frame(Elements = names(pcaResults), Classes = sapply(pcaResults, function(x)class(x)[1]), row.names = c(2,3,4,5,1)) # make a table knitr::kable(pcaResults_table[order(rownames(pcaResults_table)),],caption = &quot;PCA output classes&quot;) Table 4.2: PCA output classes Elements Classes x matrix sdev numeric rotation matrix center numeric scale numeric PCA gives you a list. A list in R can be thought of a database that keep many different type of information for you (called elements). You can see what type of information is in the result (list) of PCA in table 4.2. The single elemnts of PCA list can be accessed using the dollar sign ($) which comes immediately after the name of the list. For example if you want to see the values of x, it can be accessed by pcaResults$x Or rotations can be accessed by pcaResults$rotation In the next sections we will use these elements to investigate our data. 4.5 Visualization of data and dimentionality reduction One of primary aspect of pattern recognition is to visually look at the data and see how the observations (samples) have been spread compared to each other. Do we see our treatment to be well separated from our control groups (potentially indicating a treatment effect)? Do we see a gradient of sample spread with respect to dose of a drug we gave to the patients? Do we have any unnoticed experimental bias? The list of questions can go on and on and it’s pretty much impossible to list them all. The key point here is that for the reasons that we previously mentioned, it’s difficult ot look at the original data mainly due to large number of variables and in addition maybe the effect of interest is not “caused” by one variable but a combinition of different variables. We can use PCA (or similar methods) to squeeze the data into a set of limited variables that is easier to look at. These new variables are in x element of our pca result. These are normally called PC scores or variates or x. Let’s have a look at this: # make a table knitr::kable(pcaResults$x[,1:5], caption = &quot;PCA output Scores (only top 5 are shown)&quot;) Table 4.3: PCA output Scores (only top 5 are shown) PC1 PC2 PC3 PC4 PC5 Sample1_Group1_Batch1 -142.4977 -20.151157 7.364239 -23.7018049 4.9708775 Sample2_Group1_Batch1 -135.2110 -50.462934 43.692008 -4.1229620 -1.9423228 Sample3_Group1_Batch1 -139.0443 -61.125761 -4.613422 38.4579480 -22.1566490 Sample4_Group1_Batch1 -140.9501 8.041048 17.950046 -37.9425775 15.5112450 Sample5_Group3_Batch1 -139.9421 36.752186 21.762929 18.2192019 4.6907520 Sample6_Group3_Batch1 -143.9337 -13.608741 1.569605 36.2175322 12.1779967 Sample7_Group3_Batch1 -133.4457 7.473995 6.967519 32.2483403 56.7414888 Sample8_Group3_Batch1 -141.5472 16.808439 12.889779 11.4550017 0.0100097 Sample9_Group3_Batch1 -139.4096 41.871771 4.580464 -3.7737890 6.0779133 Sample10_Group2_Batch1 -146.9726 -11.140862 -38.872539 -21.4178914 -11.5489616 Sample11_Group2_Batch1 -142.0313 2.452566 20.086084 -8.0942913 -29.6993857 Sample12_Group2_Batch1 -141.3450 -22.237362 9.148693 -1.5475102 0.9198088 Sample13_Group2_Batch1 -143.9038 -1.859245 2.014499 -26.7160541 -10.9344309 Sample14_Group4_Batch1 -147.4363 35.800426 -38.092775 -12.0340459 -0.2656263 Sample15_Group4_Batch1 -139.3653 30.648049 -64.715916 4.0801686 -23.3560720 Sample16_Group1_Batch2 266.2316 -38.382633 -28.188729 -9.7919721 38.3088649 Sample4_Group1_Batch2 264.8721 6.932182 50.200805 -47.9462872 -2.1211571 SampleA_Group2_Batch2 263.1922 -21.574565 -32.166062 -16.5126360 14.7941615 Sample10_Group2_Batch2 264.0484 -40.878854 -34.106663 2.0871555 -15.0694882 Sample5_Group3_Batch2 264.1643 6.182041 23.738773 28.2694399 -22.8995487 SampleB_Group4_Batch2 263.8839 41.230650 -12.071945 0.3629786 0.8558170 SampleCGroup4_Batch2 265.4505 17.661788 26.862815 19.4137281 -20.7695081 SampleD_Group4_Batch2 265.1927 29.566972 3.999793 22.7903267 5.7042151 You can see in Table 4.3 that the observation are in rows and the new variables (new axis) are in columns. The total number of new axis can be maximum of 23 that is the total number of observations we have in the dataset. So we have 23 new variables. If you remember, we said that PCA sort these new variables so that the first one is the one showing most of the variance. The seocnd one is showing the next largest variance and so forth. You can see the standard deviation of these new axis in pcaResults$sdev Each element of pcaResults\\(sdev* correspond to the standard deviation of the same variable in *pcaResults\\)x. This means pcaResults\\(sdev[1]* is the standard deviation of the first column of *pcaResults\\)x. The same thing for the second one, pcaResults\\(sdev[2]* is the standard deviation of the second column of *pcaResults\\)x. Here we refer to each column of x as component. So component 1 means the new variable 1 that has been created by PCA. Intuitively, as we are interested in most of the variation, we go ahead and plot the first two components of PCA. # set number of figures par(mfrow=c(1,1)) # calculate variation explained x.var &lt;- pcaResults$sdev ^ 2 x.pvar &lt;- x.var/sum(x.var) x.pvar&lt;-x.pvar*100 # plot the first two components plot(pcaResults$x[,1:2],xlab=paste(&quot;PC1, var.exp:&quot;,round(x.pvar[1]),&quot;percent&quot;), ylab=paste(&quot;PC2, var.exp:&quot;,round(x.pvar[2]),&quot;percent&quot;)) Figure 4.4: PCA of the data using first two components Amazing. This is our PCA results. Here in Figure ’@ref(fig:pcaexample1), the x axis is component 1 and the y axis is component 2 of the PCA. We can see that the first compoent alone shows us 87 percent of the total variation in our dataset. This is amazing! we have had 45101 variables with a total of 3.034990810^{4} variation. Now we can see 87 percent of the variation (2.628180310^{4}) in only one variable! And look, the second component only explains 2 percent of the total variation so it’s not even that important compared to the fist one. But hold on, we first need to see what this new variable actually shows us? Here the statistics does not help much, we as researcher should make sense of this component. By adding shapes, colors etc to the figure, we can start making sense this. Let’s add our group information first: # set number of figures par(mfrow=c(1,1)) # calculate variation explained x.var &lt;- pcaResults$sdev ^ 2 x.pvar &lt;- x.var/sum(x.var) x.pvar&lt;-x.pvar*100 # plot the first two components plot(pcaResults$x[,1:2],xlab=paste(&quot;PC1, var.exp:&quot;,round(x.pvar[1]),&quot;percent&quot;), ylab=paste(&quot;PC2, var.exp:&quot;,round(x.pvar[2]),&quot;percent&quot;),col=factor(metadata$Covariate)) # add legend legend(&quot;top&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) Figure 4.5: PCA of the data using first two components Well, OK. Perhaps not something that we actually expected. The largest variation in our data is not coming from difference between our biological groups (e.g. cancer sub-types). Our group of interest is only visible in the component 2. Remember, we discussed that PCA is great for finding patterns, the ones that we don’t expect, in an unbiased manner! There is something else in our data that we ignore! That is: # set number of figures par(mfrow=c(1,1)) # calculate variation explained x.var &lt;- pcaResults$sdev ^ 2 x.pvar &lt;- x.var/sum(x.var) x.pvar&lt;-x.pvar*100 # plot the first two components plot(pcaResults$x[,1:2],xlab=paste(&quot;PC1, var.exp:&quot;,round(x.pvar[1]),&quot;percent&quot;), ylab=paste(&quot;PC2, var.exp:&quot;,round(x.pvar[2]),&quot;percent&quot;),col=factor(metadata$Covariate),pch=as.numeric(factor(metadata$Batch))) # add legend legend(&quot;center&quot;, legend=paste(&quot;Batch&quot;,c(unique(levels(factor(metadata$Batch))))), pch=unique(as.numeric(factor(metadata$Batch))), cex=0.8) # add legend legend(&quot;top&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) Figure 4.6: PCA of the data using first two components (v2) Exactly. We have such a large batch effect between our samples. That is our largest variation. What should we do. Well, we can ignore it go to the next components. Let’s go up to ten components: # set number of figures par(mfrow=c(1,1)) # calculate variation explained x.var &lt;- pcaResults$sdev ^ 2 x.pvar &lt;- x.var/sum(x.var) x.pvar&lt;-x.pvar*100 # plot the first two components pairs(pcaResults$x[,c(2:10)], col=factor(metadata$Covariate),pch=as.numeric(factor(metadata$Batch))) Figure 4.7: PCA of the data using up to ten components In Figure 4.7, we have plotted all possible pairs of components from 2 to 10. This might look scary a bit but If we take a plot in Figure 4.7 which has data points in it, we can figure out what is the x axis but just going up and down and look what “PC” is on that column. The same for the y axis. We can just go left and right and figure out which PC is in that row. The combination of PC2 and PC9 looks interesting as they show some group differences. Let’s plot them: # set number of figures par(mfrow=c(1,1)) # calculate variation explained x.var &lt;- pcaResults$sdev ^ 2 x.pvar &lt;- x.var/sum(x.var) x.pvar&lt;-x.pvar*100 # plot the first 2 and 9 components plot(pcaResults$x[,c(2,9)],xlab=paste(&quot;PC2, var.exp:&quot;,round(x.pvar[2]),&quot;percent&quot;), ylab=paste(&quot;PC9, var.exp:&quot;,round(x.pvar[3]),&quot;percent&quot;),col=factor(metadata$Covariate), pch=as.numeric(factor(metadata$Batch))) # add legend legend(&quot;top&quot;, legend=c(unique(levels(factor(metadata$Covariate)))), col=unique(as.numeric(factor(metadata$Covariate))), cex=0.8, pch = 1) # add legend legend(&quot;topleft&quot;, legend=paste(&quot;Batch&quot;,c(unique(levels(factor(metadata$Batch))))), pch=unique(as.numeric(factor(metadata$Batch))), cex=0.8) Figure 4.8: PCA of the data 2 and 9 components That is not that bad! We can see differences between groups of interest and not batch effect. So we can conclude that there is certainly some differences between the groups but it’s much lower than our experimental factors (such as batch or others that we don’t know!). So our experiment has been successful. Please also note that, if you see the variation of interest in the first components, it’s generally a good sign (unless there is confounding somewhere) as PCA found what you wanted without knowing it, meaning that the signal has been very strong compared to noise! Now we want to move one step back and ask given the differences we see in these new variables, what can we say about the original variables (e.g. genes). Are there some genes specific responsible for these differences that we see? Or What are the genes that are influence by group (cancer sub-types) or even batches? This is the topic the next secion. 4.6 Variable selection In many experiments, we are not only interested in seeing whether the experiment has been successful, but we are also interested in know whether any of the measurements (variables) have been affected let’s say by some kind of perturbation? Variable selection is the key to answer this question. In short, we are going to single out the variables that might be of interest for further investigation. Some examples can be performing t-test or regression and deciding which variables are “statistically significant”. PCA also gives us a tool to do so. However, it does that without a need to do statistical testing. If we recall, in Table 4.3, we had another element (returned by PCA) that is called rotation. What are these? We have been repeatedly saying that PCA creates new variables that are combinations of the original variable. We can think about a combination like New variable (let’s say component 1) = original variable 1 + original variable 2 + original variable 3. However, PCA adds a little more! It adds weights to the original variables and then combine them. So the new variable is the weighted combination of the original ones. These weights in each component are indications of importance or influence of a particular variable on the data pattern we see in a component. These weight can be positive or negative, but for now, the larger the absolute value, the more interesting the variable is for us. At the end please note that these weights rotations are also called loadings in many contexts. Having this said let’s have a look at the rotation: # make a table knitr::kable(pcaResults$rotation[1:5,1:5], caption = &quot;PCA output rotation (only top 5 are shown)&quot;) Table 4.4: PCA output rotation (only top 5 are shown) PC1 PC2 PC3 PC4 PC5 0.0046596 -0.0015491 -0.0050678 -0.0013241 0.0030454 0.0044884 -0.0020155 -0.0023613 -0.0061551 -0.0025862 0.0046884 -0.0038413 0.0016769 0.0037431 0.0104244 0.0049866 -0.0010551 -0.0030093 0.0018562 0.0009776 0.0047319 -0.0029789 0.0029067 -0.0035312 0.0034596 In Table 4.4, we only show five variables (weights) for five components but in fact there are 23 components (PCs) for each we have 45101 weights. The way the we do variable selection is, for the component of interest we sort the absolute values of these weights in descending order, and the select top x highest variables (let’s say top 5 if we only want 5 variables). For this experiment, we have already select the component 2 and 9 to be important for us. Now let’s select top 5 variables for each component. # set number of figures par(mfrow=c(2,1),las=2) PC2_rotations&lt;-pcaResults$rotation[order(abs(pcaResults$rotation[,2]),decreasing = T)[1:5],2] names(PC2_rotations)&lt;-as.character(order(abs(pcaResults$rotation[,2]),decreasing = T)[1:5]) PC2_rotations&lt;-rev(abs(PC2_rotations)) barplot(PC2_rotations,horiz=TRUE,main = &quot;Component 2&quot;) # add legend # set number of figures PC9_rotations&lt;-pcaResults$rotation[order(abs(pcaResults$rotation[,9]),decreasing = T)[1:5],9] names(PC9_rotations)&lt;-as.character(order(abs(pcaResults$rotation[,9]),decreasing = T)[1:5]) PC9_rotations&lt;-rev(abs(PC9_rotations)) barplot(PC9_rotations,horiz=TRUE,main = &quot;Component 9&quot;) Figure 4.9: Selecting top five variables for components 2 and 9 We can see the name of the variable on the y axis and its contribution on the x axis. So we have 10 unique variables (the variables can sometime overlap between different components). We will have to interpret these variables together with components. Because we see differences between groups 2, 3 and 4 in the second components, variables selected for this component influence the differences between the mentioned groups. The variables selected for the component 9, will mostly influence the differences between group 1 and the rest. It will take some time to get used to this kind of interpretation but after a while it will become very convenient to do variable selection using PCA. This way, we can avoid doing statistical testing, and thus avoid multiple testing problem. In addition, we can easily remove variables to avoid for example overfitting of statistical models and many other advantages. We can have a look at the variables we selected in isolation: # set number of figures par(mfrow=c(2,5)) for(x in names(sort(PC2_rotations,decreasing = T))) { # plot the data dt&lt;-data[,as.numeric(x)] plot(x=dt,y=rep(1,nrow(data)),axes = F,ylab = &quot;&quot;,xlab = paste(&quot;PC2,&quot;,x,sep=&quot;&quot;),col=factor(metadata$Covariate)) axis(1,cex=4,pos = c(0.9,0.5) ) } for(x in names(sort(PC9_rotations,decreasing = T))) { # plot the data dt&lt;-data[,as.numeric(x)] plot(x=dt,y=rep(1,nrow(data)),axes = F,ylab = &quot;&quot;,xlab = paste(&quot;PC9,&quot;,x,sep=&quot;&quot;),col=factor(metadata$Covariate)) axis(1,cex=4,pos = c(0.9,0.5) ) } Figure 4.10: Top 5 variables per component are plotted We can see that, in case of component 2, most variable show the same trend that the component 2 showed us. However, This is not the case in the component 9 where there is only one or maybe two variables (28492 and 44067) that showed what component 9 showed. There are several reasons for this, including: PCA looks where most of the variation is without knowing about the groups. We can see that there is single observation in for example variable 14987 that is very different compared to the rest of the observation. PCA components are combinations of all the variables. Simply because we chose to select the top five variables does not mean the rest of the variables are not influencing the pattern From the PCA perspective, this variables are top ones. This does not mean that they will answer our research question. That is of course up to us what to conclude! The distances between the observations in the PCA components would have been the same as the orginal data IF you did not remove any components. Now that we decided to remove some components, the distances are not the same. By now you should have a good understanding of application PCA for dimentionality reduction and visualization. We could squeeze our dataset with 45101 variables to a few ones still showing the overall data pattern. We could also extract variables with most influence on the data for the purpose of further investigations. We could stop here and you are good to start using PCA. But there is one more thing to talk about. Let’s move to the last section of this chapter. 4.7 Batch removing (removing variation) Working with PCA components is nice, their are standarized, they have specific meaning but there are situations that we want to work on the original data. However, maybe we find a variation of interest in a few PCA components and we have to keep that variation and remove the rest. Or it also can be that we found a variation that is not of interest and we want to remove it and keep the rest. The example would the batch effect that we saw before. Maybe we want to remove batch effect from our data. PCA will give us a very power tool to do so. Let’s have another look at the previous batch effect. We also select top two variables affected by the batch effect and plot them # set number of figures par(mfrow = c(1, 2)) # calculate variation explained x.var &lt;- pcaResults$sdev^2 x.pvar &lt;- x.var/sum(x.var) x.pvar &lt;- x.pvar * 100 # plot the first two components plot(pcaResults$x[, 1:2], xlab = paste(&quot;PC1, var.exp:&quot;, round(x.pvar[1]), &quot;percent&quot;), ylab = paste(&quot;PC2, var.exp:&quot;, round(x.pvar[2]), &quot;percent&quot;), pch = as.numeric(factor(metadata$Batch))) # plot title title(&quot;PCA batch effect&quot;) # add legend legend(&quot;top&quot;, legend = paste(&quot;Batch&quot;, c(unique(levels(factor(metadata$Batch))))), pch = unique(as.numeric(factor(metadata$Batch))), cex = 0.8) # PC1_rotations &lt;- pcaResults$rotation[order(abs(pcaResults$rotation[, 1]), decreasing = T)[1:2], 1] names(PC1_rotations) &lt;- (order(abs(pcaResults$rotation[, 1]), decreasing = T)[1:2]) plot(data[, (order(abs(pcaResults$rotation[, 1]), decreasing = T)[1:2])], xlab = paste(&quot;Variable&quot;, names(PC1_rotations)[1]), ylab = paste(&quot;Variable&quot;, names(PC1_rotations)[2]), pch = as.numeric(factor(metadata$Batch))) title(&quot;Original variables&quot;) Figure 4.11: PCA of the data showing the batch effect in our data Wow! Look at how the variables have been affected by batch effect. Now we do a trick (we will go through this in detail later) and ignore the component one and tell PCA to rebuild our dataset without that component but including all other components. # set number of figures par(mfrow=c(1,2)) # Reconstruct the data without the first component new_data&lt;-pcaResults$x[,-1]%*%t(pcaResults$rotation[,-1]) # rescale and recenter the data new_data&lt;-scale(new_data,scale = 1/pcaResults$scale,center = -1*pcaResults$center) # do another PCA pcaResults_new&lt;-prcomp(new_data,center = T,scale. = T) # calculate variation explained x.var &lt;- pcaResults_new$sdev ^ 2 x.pvar &lt;- x.var/sum(x.var) x.pvar&lt;-x.pvar*100 # plot the first two components plot(pcaResults_new$x[,1:2],xlab=paste(&quot;PC1, var.exp:&quot;,round(x.pvar[1]),&quot;percent&quot;), ylab=paste(&quot;PC2, var.exp:&quot;,round(x.pvar[2]),&quot;percent&quot;),pch=as.numeric(factor(metadata$Batch))) # plot title title(&quot;PCA batch effect removed&quot;) # add legend legend(&quot;top&quot;, legend=paste(&quot;Batch&quot;,c(unique(levels(factor(metadata$Batch))))), pch=unique(as.numeric(factor(metadata$Batch))), cex=0.8) plot(new_data[,(order(abs(pcaResults$rotation[,1]),decreasing = T)[1:2])],xlab=paste(&quot;Variable&quot;,names(PC1_rotations)[1]), ylab=paste(&quot;Variable&quot;,names(PC1_rotations)[2]),pch=as.numeric(factor(metadata$Batch))) title(&quot;Original variables&quot;) Figure 4.12: PCA of the data showing the removal of batch effect in our data Isn’t this beautiful!? What we did in this plot (Figure 4.12 ) was rebuilding (called reconstruction) of our data without the variation that we are not interested in and did another PCA to check where most of the variation is in the new data (the plot on the left)! We also plotted the same variables that we plotted in Figure 4.11. We clearly see that the batch effect has gone. We can use the very same method to remove/keep any variation or data pattern. We can also use the same method to decrease the size of images etc etc. That was it for the application PCA. We are going to do some simple exercises in the next chapter and then jump into the mathematical details of PCA. "],["exer.html", "Chapter 5 Exercises 5.1 Data 5.2 Exercise 1 5.3 Exercise 2 5.4 Exercise 3 5.5 Exercise 4", " Chapter 5 Exercises Welcome to the exercises! We are going to do three exercises based on a dataset that is already available in you R environment. We are not providing answer to the exercises. You will have to find them yourself. 5.1 Data The dataset is called iris. It’s coming default with R. According to R help page this is: This famous (Fisher&#39;s or Anderson&#39;s) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginic You can simply write iris in your R console to make sure you have the dataset. If you see an error, try data(iris) The dataset contains 5 columns corresponding to four variables and one grouping information. PCA only works on the numberical data. You can extract the four numerical variables using iris[,1:4] The grouping information can be extracted using iris[,5] Now you have all you need! 5.2 Exercise 1 How does the data look? Do you see any pattern!? Any differences between groups? Produce a plot showing the differences between the groups. You will have to do a pca and make a plot of PCA scores (components) 5.3 Exercise 2 What is the most important variable in component 4? What does it show? What is the meaning of it? 5.4 Exercise 3 What is the most important variable affected by the species of lowers? 5.5 Exercise 4 Remove the first component and reconstruct the data. Now repeat exercises 1 to 3! Goog luck! "],["pcamath.html", "Chapter 6 PCA (mathematical details) 6.1 Summary of the previous chapter 6.2 Foundations 6.3 Operations on vectors 6.4 Matrices", " Chapter 6 PCA (mathematical details) Welcome to the last section of this chapter. We are going to talk a bit about the mathematical details of PCA. It’s important that, it’s not necessary to understand the mathematical details of PCA in order to effectively apply this amazing method on your data. One can easily skip this section without having any problems when it comes to the application. So feel free to skip this if you find difficult to follow! 6.1 Summary of the previous chapter Since the beginning of this chapter, we have been talking about variance as a measure of variability in our data. We also talked a bit about covariance as a measure of concordance or redundancy in our data (or even total variance). We saw that working with high dimensional data is challenging and PCA can help us summarize these dimensions (e.g. genes) into a set of new variables (scores/components) so that one we see the pattern of data spread. The new dimensions were promised to be orthogonal (no covariance) and sorted so that the variance of the first variable is always higher than the second one and so on. We are now ready to see how PCA does this amazing calculation. 6.2 Foundations In order to understand the math behind PCA, we need to agree on a set of definitions. Definition 6.1 (Scalar) A scalar is a number. We work a lot with scalars. For example age,speed, temperature etc. Of course, scalars can have signs (positive and negative) and we can add them, subtract them, multiply etc. Definition 6.2 (Space) This might sound a bit tricky to understand. In fact the meaning of space can be very complicated. In our case, we define the space of a dataset to be the set of all variables defining the dimensions of the space and their possible values define the extend of the space. In math we define space by \\({\\rm I\\!R}^n\\) or \\({\\rm I\\!E}^n\\) where “n” means the number of dimentions. For example in our data, we have n=45101 that is equal to the number of genes we have measured, meaning that the space our data has 45101 dimensions. You can think about the extend of these dimensions to be all possible expression of each genes. For example, if we only have two genes in our data, we have \\({\\rm I\\!R}^2\\) giving us a 2 dimensional space. The size of this space starts from the lowest possible value for the expression of the genes to the highest possible value. The exact measured expression of each gene gives us the location of our observation in the space. # Select variable var1&lt;-18924 var3&lt;-18505 # plot the data for variable 1 plot(data[,c(var1,var3)],xlab = &quot;Variable 1&quot;,ylab = &quot;Variable 2&quot;, ylim = c(0,15),xlim=c(0,20)) Figure 6.1: Plot of two genes showing the space defined by these two variables In Figure 6.1 we show two variables. the value of variable 1 start from 0 and go all the way to 20 whereas the value of variable 2 starts from 0 and go to 15. So to put it in context, our space is defined by these variable and their possible values. Our observations (e.g. samples) are just some points or location in that space. However, for simplicity, we limit the extend of the values to the ones that we observed in our data and not all possible values of variable 1 and 2. Another way of thinking about the space is imagine outer space which more or less defines what exists in the universe and planets, stars and in general celestial bodies are located somewhere in the space. knitr::include_graphics(rep(&#39;https://www.jpl.nasa.gov/spaceimages/images/wallpaper/PIA22564-640x350.jpg&#39;)) Definition 6.3 (Origin) Let’s agree that the start of everything is from zero (0). Formally the location where axis of our space intersect is zero. Another way of thinking about this is if you multiple anything by zero the result will be zero! This is normally denoted by \\((0,0)\\) or sometime just \\(O\\). # plot the data for variable 1 plot(c(-2,2),c(-2,2),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot the axis axis(1,cex=4,pos = c(0,0) ) axis(2,cex=4,pos = c(0,0) ) # plot arrow and text arrows(1,1,0,0) text(c(1.1),label = &quot;Origin (0,0)&quot;) Figure 6.2: Plot of the origin Definition 6.4 (Vector) A vector is basically just a list of numbers, showing a location of a point in the space. We like to show the vectors by their name and a small arrow on top of them (\\(\\vec{a}\\)). For example, if we say \\(\\vec{a}=[5,13]\\), it means we are refering to a point in space where the value for the first axis is 2 and the second axis is 13. It’s often nice to show the vector with an arrow starting from the origin and finishing exactly at that point. # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot the point points(5,13) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-17, cex.lab=1.2,adj=1) # plot arrow and text arrows(0,0,5,13) Figure 6.3: Plotting a vector As we said, we like to put our starting point at \\((0,0)\\). Given this, we can also think about a vector as magnitude (length the arrow starting from the origin or distance to the origin) and also the direction to which the arrow is pointing. So in our case, we have a vector that is defined by going to the right along the \\(x\\) axis (red arrow in Figure 6.4) until reaching point 3 and the start going up along the \\(y\\) axis (blue arrow in Figure 6.4) to reach the final point. # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot the point points(5,13) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow and text arrows(0,0,5,13) arrows(0,0,5,0,col=&quot;Red&quot;,lty = &quot;dashed&quot;) arrows(5,0,5,13,col=&quot;blue&quot;,lty = &quot;dashed&quot;) Figure 6.4: Plotting a vector and the steps The magnitude of a vector is calculated by: \\[\\|\\vec{a}\\|=\\sqrt{x^2+y^2}\\] Where \\(x\\) and \\(y\\) are the first and the second elements of the vector. So in our case that is equal to \\(\\|\\vec{a}\\|=\\sqrt{5^2+13^2}=13.93\\). We normally like to measure the direction of the a vector based on the angle it makes with the \\(x\\) axis. \\[\\theta=\\tan^{-1}\\frac{x}{y}\\] These thing can easily be calculate in R for example by atan(y/x) # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot the point points(5,13) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow and text arrows(0,0,5,13) # draw arc plotrix::draw.arc(0,0,3,angle2=1) text(1,1,&quot;1.2&quot;,cex=0.7) Figure 6.5: Plotting a vector and the steps So by now we have the magnitude and direction of a vector. A vector also has a span. A span of a vector is a line resulted by stretching the vector from both ends to infinity. # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) segments(-500,-1300,500,1300,col = &quot;red&quot;,lty = &quot;dashed&quot;) text(13,20,&quot;span of the vector&quot;,col=&quot;red&quot;) # plot the point points(5,13) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow and text arrows(0,0,5,13) # draw circle draw.arc(0,0,3,angle2=1) text(1,1,&quot;1.2&quot;,cex=0.7) Figure 6.6: Plotting a vector and the steps Now we can go ahead and define some preliminary operations on the vector. 6.3 Operations on vectors Definition 6.5 (Scalar multiplication) If we multiply a vector by a scalar (a number) other than one (1), the resulting vector will change magnitunde by not direction. However, a vector might flip (pointing to opposite direction) but it will NEVER change the direction of its span. When we multiply a scalar to a vector, we take each element of the vector (e.g. \\(x\\) and \\(y\\)) and we multiply each of them by that scalar \\[\\vec{a} \\times j=[x\\times j,y\\times j]\\] where \\(\\vec{a}\\) is a vector and \\(j\\) is an scalar. For example if we take our vector \\(\\vec{a}=[5,13]\\) and multiply by \\(1.5\\) the results will be \\([5 \\times 1.5, 13 \\times 1.5]=[7.5,19.5]\\): # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) segments(-500,-1300,500,1300,col = &quot;red&quot;,lty = &quot;dashed&quot;) text(13,20,&quot;span of the vector&quot;,col=&quot;red&quot;) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow and text arrows(0,0,5*1.5,13*1.5,col=&quot;green&quot;) arrows(0,0,5,13) # draw circle draw.arc(0,0,3,angle2=1) text(1,1,&quot;1.2&quot;,cex=0.7) Figure 6.7: Multiplying a vector by a scalar As you see in Figure 6.7, the green vector is the result of the multiplication of the our vector by 1.5. The vector has changed magnitude (length) but not the direction. If we now multiply by \\(-1.5\\) # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) segments(-500,-1300,500,1300,col = &quot;red&quot;,lty = &quot;dashed&quot;) text(13,20,&quot;span of the vector&quot;,col=&quot;red&quot;) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow and text arrows(0,0,5*-1.5,13*-1.5,col=&quot;green&quot;) arrows(0,0,5,13) # draw circle draw.arc(0,0,3,angle2=1) text(1,1,&quot;1.2&quot;,cex=0.7) Figure 6.8: Multiplying a vector by a negative scalar You see that the green line now points to the negative direction but still stays on the same span (the red dashed line). Let’s move on to the addition. Obviously the same applies if we add a scalar to both element of the vector. Now let’s have a look at when we don’t have a scalar but have a vector. Let’s start with addition and subtraction Definition 6.6 (Vector addition and subtraction) We can add or substract two vectors only and only if they have the same number of elements. If we have \\(\\vec{a}=[x,y]\\) and \\(\\vec{b}=[p,z]\\) and the addition and subtraction are defined: \\[\\vec{c}=\\vec{a}+\\vec{b}=[x+p, y+z]\\] and \\[\\vec{c}=\\vec{a}-\\vec{b}=[x-p, y-z]\\] Obviously, we take similar element of the vector and add or subtract them resulting to a new vector. Let’s have a look at what it means using an example. Suppose we have two vectors, \\(\\vec{a}=[5,13]\\) and \\(\\vec{b}=[3,4]\\) we add them together and the result will be \\[\\vec{c}=\\vec{a}+\\vec{b}=[5+3, 13+4]=[8,17]\\] Similarly for subtraction \\[\\vec{c}=\\vec{a}+\\vec{b}=[5-3, 13-4]=[2,9]\\] Graphically and verbaly we can think about this, start with \\(\\vec{a}=[5,13]\\), from the origin go 5 to the right and 13 up. Great! Stop here. Now let’s continue with \\(\\vec{b}=[3,4]\\). Go 3 steps further to the right and then go 4 steps up! This is \\(\\vec{c}=\\vec{a}+\\vec{b}\\) Let’s look at the graphics: par(mfrow=c(1,2)) # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow and text arrows(0,0,5,13,col=&quot;green&quot;) arrows(0,0,3,4,col=&quot;red&quot;) title(&quot;Original vectors a and b&quot;) plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow and text arrows(0,0,5,13,col=&quot;red&quot;) arrows(5,13,8,17,col=&quot;green&quot;) title(&quot;Addtion and a and b&quot;) arrows(0,0,8,17,col=&quot;black&quot;) # draw circle draw.arc(0,0,3,angle2=1) text(1,1,&quot;1.2&quot;,cex=0.7) Figure 6.9: Adding two vectors We have two vectors, \\(\\vec{a}\\) is red, \\(\\vec{b}\\) is green and \\(\\vec{c}\\) is black. So the black one is the resulting vecotr. Has it changed span? Let’s look at the graphics: par(mfrow=c(1,1)) # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow arrows(0,0,8,17,col=&quot;black&quot;) # plot span segments(-500,-1300,500,1300,col = &quot;red&quot;,lty = &quot;dashed&quot;) text(13,20,&quot;span of the vector a&quot;,col=&quot;red&quot;) segments(-300,-400,300,400,col = &quot;green&quot;,lty = &quot;dashed&quot;) text(13,10,&quot;span of the vector b&quot;,col=&quot;green&quot;) segments(-800,-1700,800,1700,col = &quot;black&quot;,lty = &quot;dashed&quot;) Figure 6.10: Span of vector a, b and c You can obviously see that the span the \\(\\vec{c}\\) (the black dashed line) is not on either of \\(\\vec{a}\\) and $. We will later talk about why this is important. Perhaps by now you got the point that vectors are not just a list of numbers (like expression a gene for 10 samples) but rather a direction and magnitude. There are three more things to talk about before moving forward to the next section. First is that we can normalize a vector to have magnitude of 1 but keep its direction. This can be simply down by diving every element of the vector by its magnitude. \\[\\frac{\\vec{a}}{\\|\\vec{a}\\|}=[\\frac{x}{\\|\\vec{a}\\|},\\frac{y}{\\|\\vec{a}\\|}]\\] This will give a normalized or unit vector which the same direction but magnitude of 1. This is handy because we have a vector irrespective of the magnitude but we can multiply it with a scalar to go back to the same vector as we had before. For example, if we have \\(\\vec{a}=[3,13]\\) we can calculate its magnitude as \\(\\sqrt{3^2+13^2}=13.3\\) giving us the unit vector of \\(\\hat{a}=[\\frac{3}{13.3},\\frac{13}{13.3}]=[0.22,0.97]\\). This will come handy later when we do a PCA. par(mfrow=c(1,1)) # plot the data for variable 1 plot(c(-20,20),c(-20,20),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot the axis axis(1,cex=4,pos = c(0,0) ) title(xlab=&quot;x&quot;, line=-10, cex.lab=1.2,adj=1) axis(2,cex=4,pos = c(0,0) ) title(ylab=&quot;y&quot;, line=-16, cex.lab=1.2,adj=1) # plot arrow arrows(0,0,3,13,col=&quot;black&quot;) arrows(0,0,3/sqrt(sum(c(3,13)^2)),13/sqrt(sum(c(3,13)^2)),col=&quot;red&quot;) Figure 6.11: Unit vector of a has been depicted As you see in the Figure 6.11, the unit vector (red arrow) has the same direction as the original vector (black arrow) but it’s much smaller. In our example, we can multiple the unit vector to \\(13.3\\) to get back to the original vector as we had. The second point is that, I guess you noted that we talked about subtraction and addition but we did not talk about multiplication of two vectors. There are two ways of multiply two vectors. One is called cross product that is written as \\(\\vec{a} \\times \\vec{b}\\) and the second one is called dot product that is written as \\(\\vec{a}\\cdot\\vec{b}\\). Throughout this chapter, we will only talk about dot product not the cross product. But just to briefly mention it, cross product deals with finding an orthogonal to two other lines. That is if we say \\(\\vec{a}=\\vec{a} \\times \\vec{b}\\), it means that \\(\\vec{c}\\) is a line that is orthogonal to both \\(\\vec{a}\\) and \\(\\vec{b}\\). But Let’s ignore it and talk about the dot product. Definition 6.7 (Dot product of two vectors) The dot product of two vectors \\(\\vec{a}=[x,y]\\) and \\(\\vec{b}=[p,z]\\) is define as \\[d=\\vec{a}\\cdot\\vec{b}=x \\times p+y \\times z\\] It might look a bit fancy, but the concept is that we would like to project or mirror vector \\(\\vec{a}\\) onto the vector \\(\\vec{b}\\) (or perhaps the span of \\(\\vec{b}\\)). We take tip of the vector \\(\\vec{a}\\), draw an orthogonal line from the time to the vector \\(\\vec{b}\\), this gives us the location of vector \\(\\vec{a}\\) on \\(\\vec{b}\\), let’s call this new location \\(\\vec{d}\\), we now extend the vector \\(\\vec{d}\\) by the magnitude of \\(\\|\\vec{b}\\|\\). Ok! That was little bit too much. Let’s look at an example to make this clear. Let’s draw vector \\(\\vec{a}\\) and \\(\\vec{b}\\) par(mfrow=c(1,1)) # plot plot(c(-5,40),c(-2,10),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot arrow b arrows(0,0,20,0,col=&quot;blue&quot;) # write b text(10,-2,labels = &quot;b&quot;,col=&quot;blue&quot;) # plot arrow b arrows(0,y0 = 0,8,10,col=&quot;red&quot;) # write a text(4,10,labels = &quot;a&quot;,col=&quot;red&quot;) segments(8,10,8,3,lty = &quot;dashed&quot;) Figure 6.12: Two vectors a and b Great! Where does \\(\\vec{a}\\) land if it starts falling down? Now let’s take the tip of the \\(\\vec{a}\\) and draw a line perpendicular to \\(\\vec{b}\\). par(mfrow=c(1,1)) # plot plot(c(-5,40),c(-2,10),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot arrow b arrows(0,0,20,0,col=&quot;blue&quot;) # write b text(18,-1,labels = &quot;b&quot;,col=&quot;blue&quot;) # plot arrow b arrows(0,y0 = 0,8,10,col=&quot;red&quot;) # write a text(4,10,labels = &quot;a&quot;,col=&quot;red&quot;) segments(8,10,8,0,lty = &quot;dashed&quot;) text(20,6,labels = &quot;perpendicular line (projection line)&quot;,col=&quot;black&quot;) arrows(4,-1,8, 0,col=&quot;black&quot;) text(2,-1.5,labels = &quot;projection location&quot;,col=&quot;black&quot;) Figure 6.13: Two vectors a and b Now we know the location \\(\\vec{a}\\) on \\(\\vec{b}\\) this is exactly at 8 we can go ahead ahead and multiply this but the magnitude of \\(\\vec{b}\\) (20): par(mfrow=c(1,1)) # plot plot(c(-5,165),c(-2,10),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = F,type = &quot;n&quot;) # plot arrow b arrows(0,0,20,0,col=&quot;blue&quot;) # write b text(18,-1,labels = &quot;b&quot;,col=&quot;blue&quot;) # plot arrow b arrows(0,y0 = 0,8,10,col=&quot;red&quot;) # write a text(4,10,labels = &quot;a&quot;,col=&quot;red&quot;) segments(8,10,8,0,lty = &quot;dashed&quot;) text(50,6,labels = &quot;perpendicular line (projection line)&quot;,col=&quot;black&quot;) arrows(4,-1,8, 0,col=&quot;black&quot;) text(10,-1.5,labels = &quot;projection location&quot;,col=&quot;black&quot;) arrows(0,0,8*sqrt(sum(c(20,0)^2)),0,col=&quot;red&quot;,lty = &quot;dashed&quot;) text(150,-2,labels = &quot;dot product of a.b&quot;,col=&quot;red&quot;) Figure 6.14: Two vectors a and b So the dot product of \\(\\vec{a}\\) on \\(\\vec{b}\\) will be that dashed red line. One way to think about the whole concept is that, we have two different cells with different growth rate. We take cell a with some growth apply b growth to it. So if a is already going 2x, and b is going 3 we take 2 and make it 3 times larger! You will see soon that when we work with unit vectors (magnitude of 1), everything becomes much clearer. The last thing we want to discuss about the vector gets to the definition of space (6.2). At that point we did not know much about the vectors. We now can re-define our space as a vector space. The vector space is the set of all possible vectors that we can draw for our dimensions. So far we have been working on two dimensions and we said that we can think about each dimension as a gene or any kind of measurement of interest. Let’s consider two genes that can be either silence (zero) or have positive expressions. In reality we barely know the extend of expression of one gene. It might go from zero to some unknown number. But let’s assume that it can go all the way to 20. Now we are given all the power in world to be able to induce gene expression to create a human (based on two genes only). Since we know some good deal of linear algebra by now, it’s not that difficult! I can go ahead and say i have a person and gene 1=15 and gene 2=5. par(mfrow=c(1,1)) # plot plot(c(0,20),c(0,20),xlab = &quot;Gene 1&quot;,ylab = &quot;Gene 2&quot;, axes = T,type = &quot;n&quot;) # plot arrow b arrows(0,0,15,5,col=&quot;blue&quot;) # add point points(15,5,pch=&quot;👶&quot;) Figure 6.15: Gene expression example If we think a bit, we see that we can create so many humans by these two genes. par(mfrow=c(1,1)) # plot plot(c(0,20),c(0,20),xlab = &quot;Gene 1&quot;,ylab = &quot;Gene 2&quot;, axes = T,type = &quot;n&quot;) for(i in 1:100) { set.seed(i) g1&lt;-sample(1:20,1) set.seed(i*100) g2&lt;-sample(1:20,1) # plot arrow b arrows(0,0,g1,g2,col=&quot;blue&quot;) # add point points(g1,g2,pch=&quot;👶&quot;) } Figure 6.16: Gene expression example with more vectors These possible values, are in fact part of your vector spaces. Essentially all the vectors that can be created are located in this space. The power we were given to work with such space and create what we want were already covered. Our tools are vector addition, subtraction, scalar multiplication and possible dot product. There is only one thing left. We need two buffers, each container the two genes that we can activate/deactivate (ONE at the time), induce the expression, mix the buffers and create a human! Our First buffer we call b1 is contains both gene 1 and 2, but gene 2 is deactivated, giving us \\(b1=[gene1,gene2]\\). Let’s say we want to represent activated gene by 1 and deactivated gene by zero so b1 one becomes \\(b1=[1,0]\\). Let’s do the same thing for our second buffer, \\(b2=[gene1,gene2]\\) and turn gene1 to off, \\(b1=[0,1]\\). Let’s again say that we want to create a human (\\(h\\)) with gene1=15 and gene2=5 so our sample should \\(h=[15,5]\\). With the tools that we are given and two buffers \\(b1=[1,0]\\) and \\(b2=[0,1]\\) how can we reach to \\(h=[15,5]\\)? Well, that is not that difficult. Let’s induce gene 1 and call it \\(b1i\\): \\(b1i=15 \\times b1=[15 \\times 1, 15 \\times 0]=[15,0]\\). Perfect. We induced the first gene but scalar multiplication. Now we do this for the second gene and call it \\(b2i\\): \\(b2i=5 \\times b2=[5 \\times 0, 5 \\times 1]=[0,5]\\). We now have gene 2 again with scalar multiplication. How can we mix these two genes and make our first human: \\(h=b1i+b2i=[15+0,5+0]=[15,5]\\). Yess! We made it! These small raw vectors (\\(\\vec{b1}\\) and \\(vec{b2}\\)) together with our operations (addition, multiplication etc) help us to create any possible vector in our space. These are basis vectors. Definition 6.8 (Basis vectors) The basis vectors are a set of vectors that when used in weighted combinition can create any vector in our space. In fact our space itself is defined by the basis vectors. To be called basis vector, these should be pointing to different directions. To be more concrete, the basis vectors should be linearly independent, meaning that they cannot be created by combination of other vectors. Our “default” basis vectors are x=[1,0] and y=[0,1]. That is the only reason that when we plot any data in for examle R, the values of our measurements will be plotted exactly as they are. If we assume anything else than x=[1,0] and y=[0,1], our plot will end up being something else. So in short, the default basis vector, let us moving around our space to any point and also show us the original angle of our data. We will see later that we can change these basis vectors in order ot rotate our data and see another angle of them. So far let’s agree that our basis are [0,1] and [1,0] and they are used to construct our data vectors. We can think about this, We with the help of our data change direction, and magnitude of the basis vectors in order to represent our data in the space defined by the basis vectors. Let’s have a look at the previous example. We start with two basis vectors \\(\\vec{b1}=[1,0]\\) (blue arrow) and \\(\\vec{b2}=[0,1] (green arrow)\\). Our aim was to reach the vector \\(\\vec{h}=[15,5]\\) (black point): par(mfrow=c(1,1)) # plot plot(c(-1,15),c(-1,10),xlab = &quot;Gene 1&quot;,ylab = &quot;Gene 2&quot;, axes = T,type = &quot;n&quot;) arrows(0,0,1,0,col=&quot;blue&quot;,length = 0.1) text(0.5,-0.5,&quot;b1&quot;,col=&quot;blue&quot;) arrows(0,0,0,1,col=&quot;red&quot;,length = 0.1) text(-0.5,0.5,&quot;b2&quot;,col=&quot;red&quot;) points(15,5) Figure 6.17: Example of basis vectors We first calculate this: \\(b1i\\): \\(b1i=15 \\times b1=[15 \\times 1, 15 \\times 0]=[15,0]\\), giving us the dashed arrow: par(mfrow=c(1,1)) # plot plot(c(-1,15),c(-1,10),xlab = &quot;Gene 1&quot;,ylab = &quot;Gene 2&quot;, axes = T,type = &quot;n&quot;) arrows(0,0,15,0,col=&quot;blue&quot;,length = 0.1,lty = &quot;dashed&quot;) text(7,-0.5,&quot;b1i&quot;,col=&quot;blue&quot;) arrows(0,0,0,1,col=&quot;red&quot;,length = 0.1) text(-0.5,0.5,&quot;y&quot;,col=&quot;red&quot;) points(15,5) Figure 6.18: Example of basis vectors (scale b1) We then calculate \\(b2i\\): \\(b2i=5 \\times b2=[5 \\times 0, 5 \\times 1]=[0,5]\\): par(mfrow=c(1,1)) # plot plot(c(-1,15),c(-1,10),xlab = &quot;Gene 1&quot;,ylab = &quot;Gene 2&quot;, axes = T,type = &quot;n&quot;) arrows(0,0,15,0,col=&quot;blue&quot;,length = 0.1,lty = &quot;dashed&quot;) text(7,-0.5,&quot;b1i&quot;,col=&quot;blue&quot;) arrows(0,0,0,5,col=&quot;red&quot;,length = 0.1,lty = &quot;dashed&quot;) text(-0.5,2.5,&quot;b2i&quot;,col=&quot;red&quot;) points(15,5) Figure 6.19: Example of basis vectors (scale b2) We can now calculate \\(h=b1i+b2i=[15+0,5+0]=[15,5]\\): par(mfrow=c(1,1)) # plot plot(c(-1,15),c(-1,10),xlab = &quot;Gene 1&quot;,ylab = &quot;Gene 2&quot;, axes = T,type = &quot;n&quot;) arrows(0,0,15,0,col=&quot;blue&quot;,length = 0.1,lty = &quot;dashed&quot;) text(7,-0.5,&quot;b1i&quot;,col=&quot;blue&quot;) arrows(15,0,15,5,col=&quot;red&quot;,length = 0.1,lty = &quot;dashed&quot;) text(14.5,2.5,&quot;b2i&quot;,col=&quot;red&quot;) points(15,5) Figure 6.20: Example of basis vectors (b1i+b2i) We can now draw a line from the origin to the tip of \\(b1i+b2i\\): par(mfrow=c(1,1)) # plot plot(c(-1,15),c(-1,10),xlab = &quot;Gene 1&quot;,ylab = &quot;Gene 2&quot;, axes = T,type = &quot;n&quot;) arrows(0,0,15,0,col=&quot;blue&quot;,length = 0.1,lty = &quot;dashed&quot;) text(7,-0.5,&quot;b1i&quot;,col=&quot;blue&quot;) arrows(15,0,15,5,col=&quot;red&quot;,length = 0.1,lty = &quot;dashed&quot;) text(14.5,2.5,&quot;b2i&quot;,col=&quot;red&quot;) arrows(0,0,15,5,col=&quot;black&quot;,length = 0.1,lty = &quot;dashed&quot;) text(8,3.5,&quot;h&quot;,col=&quot;black&quot;) points(15,5) Figure 6.21: Example of basis vectors (calculating h) I hope that you got the point by now. We can use the basis vectors to construct any vector in our space. In fact, our observations vectors (e.g. samples) are basis vectors that have been scaled and added. We come to the end of the vector section. The important point here is that we have been working on two dimensions, all the operations and concept discussed so far can be extend to work in high dimension. Remember our dataset had 45101 dimensions. We have only worked on two of them so far :) 6.4 Matrices Matrices can be thought as generalization of vectors where we put multiple vectors together to form a “table”. You can think about an Excel sheet where each row or each column is vector but the complete sheet is a matrix. We normally store our data in matrices as we showed in Table 4.1. As you noticed, matrices have can have columns and rows. These are dimensions of a matrix. In our test data, we dimension is # show dimensions of the data dim(data) ## [1] 23 45101 Throughout this section, we show matrices with capital letters as opposed to vectors that are shown using small letters. \\[ A=\\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix} \\] As you see, we have two rows and three columns. When we want to refer to a specific location (called entry) in the matrix, we use i for the row index and j for the column index. For example \\(A_{1,2}\\) refers to an entry sitting in the first frow and the **second column of the matrix, giving us \\(2\\). We refer to the entire row of the matrix by i,*. For example, \\(A_{2,*}\\) will give us \\([4,5,6]\\). The same way, we can refer to the entire column \\(A_{*,2}\\), giving us \\([2,5]\\). Many of the operations defined for the vectors are also applied on the matrices. We can add or subtract them but the important thing is that the matrices MUST be the same size (same dimensions) to be able to perform addition and subtraction. In the case of addition and subtraction, each entry of the first matrix will be added or substracted to the same entry (same location) in the second matrix: \\[ \\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix}+ \\begin{bmatrix} 7 &amp; 8 &amp; 9\\\\ 10 &amp; 11 &amp; 12 \\end{bmatrix}= \\begin{bmatrix} 1+7 &amp; 2+8 &amp; 3+9\\\\ 4+10 &amp; 5+11 &amp; 6+12 \\end{bmatrix} \\] The same applies in scalar multiplication: \\[ 5 \\times \\begin{bmatrix} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6 \\end{bmatrix}= \\begin{bmatrix} 5 \\times 1 &amp; 5 \\times 2 &amp; 5 \\times 3\\\\ 5 \\times 4 &amp; 5 \\times 5 &amp; 5 \\times 6 \\end{bmatrix} \\] We can think about addition, subtraction and scalar multiplication the same way as the vectors. However, instead of having one vector, we have multiple ones. For example, if we add \\(A=\\begin{bmatrix}2&amp;4\\\\5&amp;3\\end{bmatrix}\\) and \\(B=\\begin{bmatrix}4&amp;5\\\\2&amp;2\\end{bmatrix}\\) we will get \\(C=\\begin{bmatrix}6&amp;9\\\\7&amp;5\\end{bmatrix}\\). We can think about we have two genes (number of columns) and we have two patients (number of rows) in \\(A\\) and \\(B\\). Each patient will give us an arrow. # par(mfrow=c(2,2)) # define matrices A&lt;-matrix(c(2,4,5,3),nrow = 2,ncol = 2,byrow = T) B&lt;-matrix(c(4,5,2,2),nrow = 2,ncol = 2,byrow = T) C=A+B # Plot arrows in A plot(c(0,10),c(0,10),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = T,type = &quot;n&quot;) title(&quot;Matrix A&quot;) arrows(0,0,A[1,1],A[1,2],col=&quot;blue&quot;,length = 0.1) text(A[1,1]/2,A[1,2]/2,expression(&#39;A&#39;[&quot;1,*&quot;]),col=&quot;blue&quot;,adj = -0.5) arrows(0,0,A[2,1],A[2,2],col=&quot;red&quot;,length = 0.1) text(A[2,1]/2,A[2,2]/2,expression(&#39;A&#39;[&quot;2,*&quot;]),col=&quot;red&quot;,adj = -0.5) # Plot arrows in B plot(c(0,10),c(0,10),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = T,type = &quot;n&quot;) title(&quot;Matrix B&quot;) arrows(0,0,B[1,1],B[1,2],col=&quot;green&quot;,length = 0.1) text(B[1,1]/2,B[1,2]/2,expression(&#39;B&#39;[&quot;1,*&quot;]),col=&quot;green&quot;,adj = -0.5) arrows(0,0,B[2,1],B[2,2],col=&quot;purple&quot;,length = 0.1) text(B[2,1]/2,B[2,2]/2,expression(&#39;B&#39;[&quot;2,*&quot;]),col=&quot;purple&quot;,adj = -0.5) # Plot arrows in A+b plot(c(0,10),c(0,10),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = T,type = &quot;n&quot;) title(&quot;Matrix A+B&quot;) arrows(0,0,A[1,1],A[1,2],col=&quot;blue&quot;,length = 0.1) text(A[1,1]/2,A[1,2]/2,expression(&#39;A&#39;[&quot;1,*&quot;]),col=&quot;blue&quot;,adj = -0.5) arrows(0,0,A[2,1],A[2,2],col=&quot;red&quot;,length = 0.1) text(A[2,1]/2,A[2,2]/2,expression(&#39;A&#39;[&quot;2,*&quot;]),col=&quot;red&quot;,adj = -0.5) arrows(A[1,1],A[1,2],C[1,1],C[1,2],col=&quot;green&quot;,length = 0.1,lty = &quot;dashed&quot;) text(C[1,1]/2,C[1,2]/2,(expression(&#39;B&#39;[&quot;1,*&quot;])),col=&quot;green&quot;) arrows(A[2,1],A[2,2],C[2,1],C[2,2],col=&quot;purple&quot;,length = 0.1,lty = &quot;dashed&quot;) text(C[2,1]/2,C[2,2]/2,(expression(&#39;B&#39;[&quot;2,*&quot;])),col=&quot;purple&quot;,adj = -2) ## plot C plot(c(0,10),c(0,10),xlab = &quot;&quot;,ylab = &quot;&quot;, axes = T,type = &quot;n&quot;) title(&quot;Matrix C&quot;) arrows(0,0,C[1,1],C[1,2],col=&quot;blue&quot;,length = 0.1) text(C[1,1]/2,C[1,2]/2,expression(&#39;C&#39;[&quot;1,*&quot;]),col=&quot;blue&quot;,adj = -0.5) arrows(0,0,C[2,1],C[2,2],col=&quot;red&quot;,length = 0.1) text(C[2,1]/2,C[2,2]/2,expression(&#39;C&#39;[&quot;2,*&quot;]),col=&quot;red&quot;,adj = -0.5) Figure 6.22: Example of matrix addition So what we see in Figure 6.22 is that, we have two matrices A and B, we take the vectors in B and put them at the tip of the vectors in A, we then draw two lines from the origin to the new tips of B. So as it’s obvious now, this is the same and vector addition we just do it for more vectors. 6.4.1 Matrix multiplication In order to understand matrix multiplication, we have to think about the space again. If you remember, we said the we can represent our space by the basis vector \\(\\vec{x}=[1,0]\\) and \\(\\vec{y}=[0,1]\\). We can think about this as directions or our guide to move to any points in the space. We said that we can scale them, we can add/subtract them to represent any vectors in the space. Our space is an empty huge place wherein we just put our vectors! The way that we navigate in space, is just by scaling and adding and subtracting the basis vectors. I keep repeating this because it’s important to understand it thoroughly. So let’s have a look at our empty space (well, i just put the grid so we have feeling of it) par(mfrow=c(1,1)) mydata&lt;-cbind(-5:5,rep(-5,11),-5:5,rep(5,11)) mydata2&lt;-cbind(rep(-5,11),-5:5,rep(5,11),-5:5) plot(range(rbind(mydata[,1],mydata[,3])),range(rbind(mydata[,2],mydata[,4])), type=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,xlim=c(-5,5),ylim=c(-5,5),axes = F,bg=&quot;black&quot;) nl&lt;-apply(mydata,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) nl&lt;-apply(mydata2,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) arrows(0,0,1,0,col=&quot;red&quot;,length=0.1) arrows(0,0,0,1,col=&quot;blue&quot;,length=0.1) Figure 6.23: Empty space We can go to any point on this grid, simply using our basis vectors. par(mfrow=c(1,1)) mydata&lt;-cbind(-5:5,rep(-5,11),-5:5,rep(5,11)) mydata2&lt;-cbind(rep(-5,11),-5:5,rep(5,11),-5:5) plot(range(rbind(mydata[,1],mydata[,3])),range(rbind(mydata[,2],mydata[,4])), type=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,xlim=c(-5,5),ylim=c(-5,5),axes = F,bg=&quot;black&quot;) nl&lt;-apply(mydata,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) nl&lt;-apply(mydata2,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) arrows(0,0,1,0,col=&quot;red&quot;,length=0.1) arrows(0,0,0,1,col=&quot;blue&quot;,length=0.1) points(4,3,col=&quot;purple&quot;,pch=7) Figure 6.24: Empty space and a target point Remember now, this target point can be an observation (e.g. sample etc). We can move to this point simply by multiplying \\(\\vec{x}\\) to 4 and \\(\\vec{y}\\) to 3 and then add the resulting vectors, giving us the location of the purple point \\([4,3]\\). par(mfrow=c(1,1)) mydata&lt;-cbind(-5:5,rep(-5,11),-5:5,rep(5,11)) mydata2&lt;-cbind(rep(-5,11),-5:5,rep(5,11),-5:5) plot(range(rbind(mydata[,1],mydata[,3])),range(rbind(mydata[,2],mydata[,4])), type=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,xlim=c(-8,8),ylim=c(-8,8),axes = F,bg=&quot;black&quot;) nl&lt;-apply(mydata,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) nl&lt;-apply(mydata2,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) arrows(0,0,1,0,col=&quot;red&quot;,length=0.1,) arrows(0,0,0,1,col=&quot;blue&quot;,length=0.1) points(4,3,col=&quot;purple&quot;,pch=7) arrows(0,0,4,3,col=&quot;purple&quot;,length=0.1) Figure 6.25: Empty space and a target point So far we have only changed the vectors. But we can also change the space itself. If we multiply a matrix to the space itself, we change the space. You can think about this like stretching, squeezing etc applied on the whole grid and ALL the things on that grid. It might not be very understandable in the beginning. Let’s have a look an example. par(mfrow=c(1,1)) mydata&lt;-cbind(-5:5,rep(-5,11),-5:5,rep(5,11)) mydata2&lt;-cbind(rep(-5,11),-5:5,rep(5,11),-5:5) plot(range(rbind(mydata[,1],mydata[,3])),range(rbind(mydata[,2],mydata[,4])), type=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,xlim=c(-8,8),ylim=c(-8,8),axes = F,bg=&quot;black&quot;) b&lt;-as.matrix(data.frame(a=c(1,0),b=c(1,2))) mydata[,c(1,2)]&lt;-t(b%*%t(mydata[,c(1,2)])) mydata[,c(3,4)]&lt;-t(b%*%t(mydata[,c(3,4)])) mydata2[,c(1,2)]&lt;-t(b%*%t(mydata2[,c(1,2)])) mydata2[,c(3,4)]&lt;-t(b%*%t(mydata2[,c(3,4)])) nl&lt;-apply(mydata,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) nl&lt;-apply(mydata2,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) Figure 6.26: Multiply a matrix to space In figure 6.26, we multiply \\(\\begin{bmatrix}1&amp;1\\\\0&amp;2\\end{bmatrix}\\) to our space. Please remember, we agreed before that our space was defined by \\(\\vec{x}=[1,0]\\) and \\(\\vec{y}=[0,1]\\) and for clarification, we plotted all the parallel lines (grid) to \\(\\vec{x}=[1,0]\\) and \\(\\vec{y}=[0,1]\\). Isn’t this awesome?! We can transform the space itself. We are in a transformed world! How do we find the new address (location) of the previous point (the purple one) in this new world? Well, the only thing we need, is our guides. The new location of the basis vectors. Once we get them, we can use them to find the new address. Let’s find them: par(mfrow=c(1,1)) mydata&lt;-cbind(-5:5,rep(-5,11),-5:5,rep(5,11)) mydata2&lt;-cbind(rep(-5,11),-5:5,rep(5,11),-5:5) plot(range(rbind(mydata[,1],mydata[,3])),range(rbind(mydata[,2],mydata[,4])), type=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,xlim=c(-8,8),ylim=c(-8,8),axes = F,bg=&quot;black&quot;) b&lt;-as.matrix(data.frame(a=c(1,0),b=c(1,2))) mydata[,c(1,2)]&lt;-t(b%*%t(mydata[,c(1,2)])) mydata[,c(3,4)]&lt;-t(b%*%t(mydata[,c(3,4)])) mydata2[,c(1,2)]&lt;-t(b%*%t(mydata2[,c(1,2)])) mydata2[,c(3,4)]&lt;-t(b%*%t(mydata2[,c(3,4)])) nl&lt;-apply(mydata,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) nl&lt;-apply(mydata2,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) arrows(0,0,1,0,col=&quot;red&quot;,length=0.1,) arrows(0,0,1,2,col=&quot;blue&quot;,length=0.1) Figure 6.27: Multiply a matrix to space, including the basis vectors Now we have them. Great. In this new world, \\(\\vec{x}=[1,0]\\) so unchanged but \\(\\vec{y}=[1,2]\\). Now we use the same address as before. We said that we multiple \\(\\vec{x}\\) to 4, so it becomes, \\(\\vec{x}=[4,0]\\) and we multiply \\(\\vec{y}\\) to 3 so it becomes \\(\\vec{y}=[3,6]\\). Now we add them \\(\\vec{x}+\\vec{y}=[7,6]\\). This is the address of the same point in the new world! par(mfrow=c(1,1)) mydata&lt;-cbind(-5:5,rep(-5,11),-5:5,rep(5,11)) mydata2&lt;-cbind(rep(-5,11),-5:5,rep(5,11),-5:5) plot(range(rbind(mydata[,1],mydata[,3])),range(rbind(mydata[,2],mydata[,4])), type=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,xlim=c(-8,8),ylim=c(-8,8),axes = F,bg=&quot;black&quot;) b&lt;-as.matrix(data.frame(a=c(1,0),b=c(1,2))) mydata[,c(1,2)]&lt;-t(b%*%t(mydata[,c(1,2)])) mydata[,c(3,4)]&lt;-t(b%*%t(mydata[,c(3,4)])) mydata2[,c(1,2)]&lt;-t(b%*%t(mydata2[,c(1,2)])) mydata2[,c(3,4)]&lt;-t(b%*%t(mydata2[,c(3,4)])) nl&lt;-apply(mydata,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) nl&lt;-apply(mydata2,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) arrows(0,0,(b%*%c(1,0))[1],(b%*%c(1,0))[2],col=&quot;red&quot;,length=0.1,) arrows(0,0,(b%*%c(0,1))[1],(b%*%c(0,1))[2],col=&quot;blue&quot;,length=0.1) points((b%*%c(4,3))[1],(b%*%c(4,3))[2],col=&quot;purple&quot;,pch=7) arrows(0,0,(b%*%c(4,3))[1],(b%*%c(4,3))[2],col=&quot;purple&quot;,length=0.1) Figure 6.28: Location of the target point So to put it simple, we just changed our coordinate system. The way the we use to move around space has changed. The only way to find the location of the previous point, is to know what transformation has been done to the space and what happened to our basis. Let’s have a look at another transformation. par(mfrow=c(1,1)) mydata&lt;-cbind(-5:5,rep(-5,11),-5:5,rep(5,11)) mydata2&lt;-cbind(rep(-5,11),-5:5,rep(5,11),-5:5) plot(range(rbind(mydata[,1],mydata[,3])),range(rbind(mydata[,2],mydata[,4])), type=&quot;n&quot;,xlab=&quot;&quot;,ylab=&quot;&quot;,xlim=c(-8,8),ylim=c(-8,8),axes = F,bg=&quot;black&quot;) tet&lt;-2 b&lt;-matrix(c(cos(tet),sin(tet),-sin(tet),cos(tet)),nrow = 2,byrow = T) mydata[,c(1,2)]&lt;-t(b%*%t(mydata[,c(1,2)])) mydata[,c(3,4)]&lt;-t(b%*%t(mydata[,c(3,4)])) mydata2[,c(1,2)]&lt;-t(b%*%t(mydata2[,c(1,2)])) mydata2[,c(3,4)]&lt;-t(b%*%t(mydata2[,c(3,4)])) nl&lt;-apply(mydata,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) nl&lt;-apply(mydata2,MARGIN = 1,function(x){ segments(x[1],x[2],x[3],x[4])}) arrows(0,0,(b%*%c(1,0))[1],(b%*%c(1,0))[2],col=&quot;red&quot;,length=0.1,) arrows(0,0,(b%*%c(0,1))[1],(b%*%c(0,1))[2],col=&quot;blue&quot;,length=0.1) points((b%*%c(4,3))[1],(b%*%c(4,3))[2],col=&quot;purple&quot;,pch=7) arrows(0,0,(b%*%c(4,3))[1],(b%*%c(4,3))[2],col=&quot;purple&quot;,length=0.1) Figure 6.29: Location of the target point in rotation Exactly! This transformation has rotated our space. It’s really cool. The next time you use a software to rotate your picture, you know there is math behind it!! So, We can change our empty base space to “any space” and still find the location any point in that space. Definition 6.9 (Matrix multiplication) Matrix multiplication is used to trasform the space into another shape. This multiplication is define between two matrices and will produce another matrix as the result. In order to be able to multiple two matrices, the number of columns of the first matrix (left hand side) MUST match the number of rows the second matrix. If the dimensions of the first matrix is \\(l \\times m\\) and the dimensions of the second matrix is \\(m \\times n\\), the result of the multiplication will be a matrix with dimensions of \\(l \\times n\\). To multiply two matrices \\(A\\) and \\(B\\), giving \\(C\\), we take the first row of \\(A\\) and the first column of \\(B\\), multiple the corresponding elements and sum the results. This result will be the first element in \\(C\\) with location \\([1,1]\\). We then proceed with the second column of \\(B\\) matrix and calculate its multiplication with the first row of \\(A\\), sum the results in \\(C\\) at the first row and second column [1,2]. We repeat this until all columns of the second matrix have covered. We then move to the second row of \\(A\\), we repeat this process but put the results in the second row of \\(C\\). We keep doing this until we multiply all rows of \\(A\\) by all columns of \\(B\\). Example: \\[\\begin{bmatrix} 1 &amp; 2 \\\\ 4 &amp; 5 \\end{bmatrix} \\cdot \\begin{bmatrix} 7 &amp; 8 &amp; 9\\\\ 10 &amp; 11 &amp; 12 \\end{bmatrix}\\] = \\[\\begin{bmatrix} 1 \\times 7 + 2 \\times 10&amp; 1 \\times 8 + 2 \\times 11 &amp; 1 \\times 9 + 2 \\times 12\\\\ 4 \\times 7 + 5 \\times 10&amp; 4 \\times 8 + 5 \\times 11 &amp; 4 \\times 9 + 5 \\times 12 \\end{bmatrix}\\] = \\[\\begin{bmatrix} 27 &amp; 30 &amp; 33\\\\ 78 &amp; 87 &amp; 96 \\end{bmatrix}\\] Don’t worry if it’s too much of work. You can calculate this with pretty much any statistical software. There are even websites that can help you understand and do this multiplication. But for now, let’s discuss what this matrix multiplication mean, in the context of our gene expression dataset. Considering what we have been saying, do you think our gene expression dataset changes the space? From the linear algebra perspective, yes it does! It might not be that intuitive in the beginning, but if we assume a stem cell as our basis, we then change this stem cell in a way that it represent the exact expression pattern found in a sample. We do pretty much the same thing with our spaces/vectors etc. We start with some basis vectors, a raw “meaningless” space (well, maybe biologically), we then change these to show our gene expression space. So to summarize, we have a raw space (n dimensions) that is defined by a set of unit vectors (e.g. for two dimensions: \\(\\vec{x}=[1,0]\\) and \\(\\vec{y}={0,1}\\)), we then transform this space using our dataset so we are now in our gene expression space. We do this simply and naively by data%*%diag(ncol(data)) This funny operation (%%)* will do matrix multiplication. Read verbally, take a raw space with the dimensions of my data ncol(data) and transform it into my gene expression space. What is the results of this operation if you do it in R? Well, it’s going to be exact gene expression data that you started with. So we actually don’t need to do that, mathematically, we almost always assume that our starting basis vectors are \\(\\vec{x}=[1,0]\\) and \\(\\vec{y}={0,1}\\) etc. So we are in the gene expression space already from the start. Any location/observation/vectors can be found using the combination of genes. There are a few important things to know before moving forward: Each matrix has two spaces: column space and row space I guess by now, you realized that matrices are very powerful, we can do a lot with them. The question is that what can we say about the transformation a matrix does to our space? If we have thousands of variables (thousands of dimensions),defining our space transformation, it’s difficult to every time plot and see what it does. We need to be able to at least say something about the transformation without plotting so many arrows and other stuff! Almost all the operations that we will be talking about in the rest of the chapter is around square matrices. Let’s quickly define them and go forward with the rest of the chapter. 6.4.1.1 Square matrices A Square matrix is a matrix with the same number of rows and columns. Square matrices are used in linear algebra quite a lot and are easier to work with. There are several reasons to why we want to use square matrices but to summarize all: When we multiply a matrix with \\(n\\) number of rows and \\(m\\) number columns by by another matrix with \\(m\\) rows and \\(l\\) columns, we are mapping the space from \\(m\\) to \\(n\\) which is essentially another space which might change the definitions of certain things. In many cases, we would like to stay in the same dimensions (not going to a totally different space). In these cases, square matrices are used to map from \\(n\\) to \\(n\\), meaning that it maps from a space to itself. Please note that, we are still doing the transformation to the vectors and everything else in that space, but we are in the same dimensions. Let’s look at an example. par(mfrow=c(2,2)) cat_data&lt;-read.table(&quot;data/cat.tsv&quot;) scatterplot3d::scatterplot3d(cat_data,angle = 150,xlab=&quot;&quot;,ylab=&quot;&quot;,zlab = &quot;&quot;) title(&quot;Original data&quot;) tet&lt;-3.2 b&lt;-matrix(c(1,0,0,0,cos(tet),sin(tet),0,-sin(tet),cos(tet)),nrow = 3,byrow = T) scatterplot3d::scatterplot3d(t(b%*%t(as.matrix(cat_data))),xlab=&quot;&quot;,ylab=&quot;&quot;,zlab = &quot;&quot;) title(&quot;Square matrix (rotation)&quot;) plot(t(b[-1,]%*%t(as.matrix(cat_data))),xlab=&quot;&quot;,ylab=&quot;&quot;) title(&quot;2d matrix transformation&quot;) plot(x=t(b[-c(1,2),]%*%t(as.matrix(cat_data))),y=rep(1,nrow(cat_data)),xlab=&quot;&quot;,ylab=&quot;&quot;) title(&quot;1d matrix transformation&quot;) Figure 6.30: Cat example You see in Figure 6.30 that at some point the meaning of things starts vanishing. We have volume in both of top panel but no volume in the bottom ones. Obviously, the 1d transformation totally distort our view. Can we say the cat’s tail is located in the middle of the body or the legs are almost parallel? Probably not in the 2d and 1d transformation. In any transformation, it’s nice to have the same geometric meaning of the concepts before and after performing such transformation. In addition, working on square matrices gives us some extra tools that are handy for describing the space and the transformation itself. 6.4.1.1.1 Inverse of a matrix 6.4.1.2 Rank of a matrix "],["references.html", "References", " References "]]
